{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "314\n",
      "**********\n",
      "epoch 1\n",
      "train_end\n",
      "trainn acc: 4102 6621 0.6195438755475003\n",
      "trainp acc: 5735 6621 0.8661833559885214\n",
      "6049472512\n",
      "test_end\n",
      "testn acc: 509 780 0.6525641025641026\n",
      "testp acc: 649 780 0.8320512820512821\n",
      "6049472512\n",
      "Loss: 8.232313479488319\n",
      "Loss: 3.8520520508289335\n",
      "**********\n",
      "epoch 2\n",
      "train_end\n",
      "trainn acc: 4312 6621 0.6512611388007854\n",
      "trainp acc: 5867 6621 0.8861199214620148\n",
      "6049472512\n",
      "test_end\n",
      "testn acc: 487 780 0.6243589743589744\n",
      "testp acc: 675 780 0.8653846153846154\n",
      "6049472512\n",
      "Loss: 7.6451254472153805\n",
      "Loss: 4.084273035526276\n",
      "**********\n",
      "epoch 3\n",
      "train_end\n",
      "trainn acc: 4429 6621 0.6689321854704727\n",
      "trainp acc: 5886 6621 0.8889895786135025\n",
      "6049472512\n",
      "test_end\n",
      "testn acc: 495 780 0.6346153846153846\n",
      "testp acc: 666 780 0.8538461538461538\n",
      "6049472512\n",
      "Loss: 7.39048613761914\n",
      "Loss: 4.00371974080801\n",
      "**********\n",
      "epoch 4\n",
      "train_end\n",
      "trainn acc: 4462 6621 0.6739163268388461\n",
      "trainp acc: 5909 6621 0.8924633741126718\n",
      "6049472512\n",
      "test_end\n",
      "testn acc: 508 780 0.6512820512820513\n",
      "testp acc: 675 780 0.8653846153846154\n",
      "6049472512\n",
      "Loss: 7.240416939388723\n",
      "Loss: 3.7049924823641778\n",
      "**********\n",
      "epoch 5\n",
      "train_end\n",
      "trainn acc: 4764 6621 0.7195287720888084\n",
      "trainp acc: 5903 6621 0.8915571665911494\n",
      "6049472512\n",
      "test_end\n",
      "testn acc: 566 780 0.7256410256410256\n",
      "testp acc: 681 780 0.8730769230769231\n",
      "6049472512\n",
      "Loss: 6.679308439934849\n",
      "Loss: 3.5061615255475043\n",
      "**********\n",
      "epoch 6\n",
      "train_end\n",
      "trainn acc: 5005 6621 0.7559281075366259\n",
      "trainp acc: 5912 6621 0.892916477873433\n",
      "6049472512\n",
      "test_end\n",
      "testn acc: 586 780 0.7512820512820513\n",
      "testp acc: 679 780 0.8705128205128205\n",
      "6049472512\n",
      "Loss: 6.269935828121974\n",
      "Loss: 3.2720565393567087\n",
      "**********\n",
      "epoch 7\n",
      "train_end\n",
      "trainn acc: 5098 6621 0.7699743241202235\n",
      "trainp acc: 5981 6621 0.903337864370941\n",
      "6049472512\n",
      "test_end\n",
      "testn acc: 639 780 0.8192307692307692\n",
      "testp acc: 651 780 0.8346153846153846\n",
      "6049472512\n",
      "Loss: 6.074299776543723\n",
      "Loss: 2.4976849529147147\n",
      "**********\n",
      "epoch 8\n",
      "train_end\n",
      "trainn acc: 5109 6621 0.771635704576348\n",
      "trainp acc: 5993 6621 0.9051502794139858\n",
      "6049472512\n",
      "test_end\n",
      "testn acc: 610 780 0.782051282051282\n",
      "testp acc: 653 780 0.8371794871794872\n",
      "6049472512\n",
      "Loss: 6.003400724125933\n",
      "Loss: 2.68814994558692\n",
      "**********\n",
      "epoch 9\n",
      "train_end\n",
      "trainn acc: 5057 6621 0.7637819060564869\n",
      "trainp acc: 5986 6621 0.904093037305543\n",
      "6049472512\n",
      "test_end\n",
      "testn acc: 598 780 0.7666666666666667\n",
      "testp acc: 688 780 0.882051282051282\n",
      "6049472512\n",
      "Loss: 5.979620364432966\n",
      "Loss: 3.1313183858990667\n",
      "**********\n",
      "epoch 10\n",
      "train_end\n",
      "trainn acc: 5171 6621 0.7809998489654131\n",
      "trainp acc: 6019 6621 0.9090771786739164\n",
      "6049472512\n",
      "test_end\n",
      "testn acc: 645 780 0.8269230769230769\n",
      "testp acc: 661 780 0.8474358974358974\n",
      "6049472512\n",
      "Loss: 5.757212274529377\n",
      "Loss: 2.495972764492035\n",
      "**********\n",
      "epoch 11\n",
      "train_end\n",
      "trainn acc: 5185 6621 0.7831143331822987\n",
      "trainp acc: 6007 6621 0.9072647636308715\n",
      "6049472512\n",
      "test_end\n",
      "testn acc: 650 780 0.8333333333333334\n",
      "testp acc: 653 780 0.8371794871794872\n",
      "6049472512\n",
      "Loss: 5.778047003639423\n",
      "Loss: 2.307255991101265\n",
      "**********\n",
      "epoch 12\n",
      "train_end\n",
      "trainn acc: 5236 6621 0.7908170971152394\n",
      "trainp acc: 6047 6621 0.9133061471076876\n",
      "6049472512\n",
      "test_end\n",
      "testn acc: 603 780 0.7730769230769231\n",
      "testp acc: 675 780 0.8653846153846154\n",
      "6049472512\n",
      "Loss: 5.574284746429508\n",
      "Loss: 2.9317334014177323\n",
      "**********\n",
      "epoch 13\n",
      "train_end\n",
      "trainn acc: 5230 6621 0.7899108895937169\n",
      "trainp acc: 6021 6621 0.9093792478477571\n",
      "6049472512\n",
      "test_end\n",
      "testn acc: 610 780 0.782051282051282\n",
      "testp acc: 701 780 0.8987179487179487\n",
      "6049472512\n",
      "Loss: 5.526566219783272\n",
      "Loss: 3.013186883032322\n",
      "**********\n",
      "epoch 14\n",
      "train_end\n",
      "trainn acc: 5233 6621 0.7903639933544782\n",
      "trainp acc: 6068 6621 0.9164778734330161\n",
      "6049472512\n",
      "test_end\n",
      "testn acc: 617 780 0.791025641025641\n",
      "testp acc: 688 780 0.882051282051282\n",
      "6049472512\n",
      "Loss: 5.519398454901233\n",
      "Loss: 2.9354228869080545\n",
      "**********\n",
      "epoch 15\n",
      "train_end\n",
      "trainn acc: 5232 6621 0.7902129587675578\n",
      "trainp acc: 6036 6621 0.9116447666515632\n",
      "6049472512\n",
      "test_end\n",
      "testn acc: 613 780 0.7858974358974359\n",
      "testp acc: 715 780 0.9166666666666666\n",
      "6049472512\n",
      "Loss: 5.429645077657478\n",
      "Loss: 3.1951577487587928\n",
      "**********\n",
      "epoch 16\n",
      "train_end\n",
      "trainn acc: 5285 6621 0.7982177918743393\n",
      "trainp acc: 6080 6621 0.918290288476061\n",
      "6049472512\n",
      "test_end\n",
      "testn acc: 635 780 0.8141025641025641\n",
      "testp acc: 675 780 0.8653846153846154\n",
      "6049472512\n",
      "Loss: 5.348975553225973\n",
      "Loss: 2.6181426903605463\n",
      "**********\n",
      "epoch 17\n",
      "train_end\n",
      "trainn acc: 5307 6621 0.8015405527865881\n",
      "trainp acc: 6075 6621 0.917535115541459\n",
      "6049472512\n",
      "test_end\n",
      "testn acc: 637 780 0.8166666666666667\n",
      "testp acc: 681 780 0.8730769230769231\n",
      "6049472512\n",
      "Loss: 5.301376062506998\n",
      "Loss: 2.612549734711647\n",
      "**********\n",
      "epoch 18\n",
      "train_end\n",
      "trainn acc: 5235 6621 0.790666062528319\n",
      "trainp acc: 6075 6621 0.917535115541459\n",
      "6049472512\n",
      "test_end\n",
      "testn acc: 608 780 0.7794871794871795\n",
      "testp acc: 698 780 0.8948717948717949\n",
      "6049472512\n",
      "Loss: 5.393764868984378\n",
      "Loss: 2.86594981521368\n",
      "**********\n",
      "epoch 19\n",
      "train_end\n",
      "trainn acc: 5321 6621 0.8036550370034738\n",
      "trainp acc: 6106 6621 0.9222171877359916\n",
      "6049472512\n",
      "test_end\n",
      "testn acc: 617 780 0.791025641025641\n",
      "testp acc: 691 780 0.8858974358974359\n",
      "6049472512\n",
      "Loss: 5.213880754496716\n",
      "Loss: 2.8636884942650793\n",
      "**********\n",
      "epoch 20\n",
      "train_end\n",
      "trainn acc: 5307 6621 0.8015405527865881\n",
      "trainp acc: 6065 6621 0.916024769672255\n",
      "6049472512\n",
      "test_end\n",
      "testn acc: 617 780 0.791025641025641\n",
      "testp acc: 711 780 0.9115384615384615\n",
      "6049472512\n",
      "Loss: 5.253609655504997\n",
      "Loss: 2.9556809866428377\n",
      "**********\n",
      "epoch 21\n",
      "train_end\n",
      "trainn acc: 5327 6621 0.8045612445249962\n",
      "trainp acc: 6095 6621 0.9205558072798671\n",
      "6049472512\n",
      "test_end\n",
      "testn acc: 633 780 0.8115384615384615\n",
      "testp acc: 690 780 0.8846153846153846\n",
      "6049472512\n",
      "Loss: 5.172262279967952\n",
      "Loss: 2.828532255887985\n",
      "**********\n",
      "epoch 22\n",
      "train_end\n",
      "trainn acc: 5339 6621 0.8063736595680411\n",
      "trainp acc: 6078 6621 0.9179882193022202\n",
      "6049472512\n",
      "test_end\n",
      "testn acc: 641 780 0.8217948717948718\n",
      "testp acc: 701 780 0.8987179487179487\n",
      "6049472512\n",
      "Loss: 5.215638183983223\n",
      "Loss: 2.449061150252819\n",
      "**********\n",
      "epoch 23\n",
      "train_end\n",
      "trainn acc: 5333 6621 0.8054674520465187\n",
      "trainp acc: 6074 6621 0.9173840809545386\n",
      "6049472512\n",
      "test_end\n",
      "testn acc: 625 780 0.8012820512820513\n",
      "testp acc: 691 780 0.8858974358974359\n",
      "6049472512\n",
      "Loss: 5.152081191764753\n",
      "Loss: 2.8621245071291925\n",
      "**********\n",
      "epoch 24\n",
      "train_end\n",
      "trainn acc: 5354 6621 0.8086391783718472\n",
      "trainp acc: 6078 6621 0.9179882193022202\n",
      "6049472512\n",
      "test_end\n",
      "testn acc: 631 780 0.808974358974359\n",
      "testp acc: 703 780 0.9012820512820513\n",
      "6049472512\n",
      "Loss: 5.139600365523262\n",
      "Loss: 2.875193413943052\n",
      "**********\n",
      "epoch 25\n",
      "train_end\n",
      "trainn acc: 5391 6621 0.8142274580879021\n",
      "trainp acc: 6096 6621 0.9207068418667875\n",
      "6049472512\n",
      "test_end\n",
      "testn acc: 642 780 0.823076923076923\n",
      "testp acc: 685 780 0.8782051282051282\n",
      "6049472512\n",
      "Loss: 5.025043868056296\n",
      "Loss: 2.5083014905452727\n",
      "**********\n",
      "epoch 26\n",
      "train_end\n",
      "trainn acc: 5388 6621 0.813774354327141\n",
      "trainp acc: 6094 6621 0.9204047726929467\n",
      "6049472512\n",
      "test_end\n",
      "testn acc: 610 780 0.782051282051282\n",
      "testp acc: 694 780 0.8897435897435897\n",
      "6049472512\n",
      "Loss: 5.067384105716571\n",
      "Loss: 2.9682608079910278\n",
      "**********\n",
      "epoch 27\n",
      "train_end\n",
      "trainn acc: 5367 6621 0.8106026280018124\n",
      "trainp acc: 6093 6621 0.9202537381060263\n",
      "6049472512\n",
      "test_end\n",
      "testn acc: 642 780 0.823076923076923\n",
      "testp acc: 702 780 0.9\n",
      "6049472512\n",
      "Loss: 5.098302486518119\n",
      "Loss: 2.5472450509667395\n",
      "**********\n",
      "epoch 28\n",
      "train_end\n",
      "trainn acc: 5393 6621 0.814529527261743\n",
      "trainp acc: 6108 6621 0.9225192569098324\n",
      "6049472512\n",
      "test_end\n",
      "testn acc: 641 780 0.8217948717948718\n",
      "testp acc: 696 780 0.8923076923076924\n",
      "6049472512\n",
      "Loss: 4.981400932896013\n",
      "Loss: 2.4559221461415293\n",
      "**********\n",
      "epoch 29\n",
      "train_end\n",
      "trainn acc: 5429 6621 0.8199667723908776\n",
      "trainp acc: 6082 6621 0.9185923576499019\n",
      "6049472512\n",
      "test_end\n",
      "testn acc: 616 780 0.7897435897435897\n",
      "testp acc: 713 780 0.9141025641025641\n",
      "6049472512\n",
      "Loss: 4.897147118664508\n",
      "Loss: 3.067200105190277\n",
      "**********\n",
      "epoch 30\n",
      "train_end\n",
      "trainn acc: 5434 6621 0.8207219453254795\n",
      "trainp acc: 6095 6621 0.9205558072798671\n",
      "6049472512\n",
      "test_end\n",
      "testn acc: 636 780 0.8153846153846154\n",
      "testp acc: 705 780 0.9038461538461539\n",
      "6049472512\n",
      "Loss: 4.888148843549824\n",
      "Loss: 2.636017617583275\n",
      "**********\n",
      "epoch 31\n",
      "train_end\n",
      "trainn acc: 5424 6621 0.8192115994562755\n",
      "trainp acc: 6103 6621 0.9217640839752304\n",
      "6049472512\n",
      "test_end\n",
      "testn acc: 611 780 0.7833333333333333\n",
      "testp acc: 711 780 0.9115384615384615\n",
      "6049472512\n",
      "Loss: 4.883248456886837\n",
      "Loss: 2.9885964311659334\n",
      "**********\n",
      "epoch 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_end\n",
      "trainn acc: 5440 6621 0.821628152847002\n",
      "trainp acc: 6083 6621 0.9187433922368222\n",
      "6049472512\n",
      "test_end\n",
      "testn acc: 633 780 0.8115384615384615\n",
      "testp acc: 702 780 0.9\n",
      "6049472512\n",
      "Loss: 4.833106032456947\n",
      "Loss: 2.6298003578186036\n",
      "**********\n",
      "epoch 33\n",
      "train_end\n",
      "trainn acc: 5457 6621 0.8241957408246489\n",
      "trainp acc: 6095 6621 0.9205558072798671\n",
      "6049472512\n",
      "test_end\n",
      "testn acc: 640 780 0.8205128205128205\n",
      "testp acc: 700 780 0.8974358974358975\n",
      "6049472512\n",
      "Loss: 4.811715563519075\n",
      "Loss: 2.594316924214363\n",
      "**********\n",
      "epoch 34\n",
      "train_end\n",
      "trainn acc: 5466 6621 0.8255550521069325\n",
      "trainp acc: 6131 6621 0.9259930524090016\n",
      "6051598336\n",
      "test_end\n",
      "testn acc: 619 780 0.7935897435897435\n",
      "testp acc: 712 780 0.9128205128205128\n",
      "6052110336\n",
      "Loss: 4.733756887718914\n",
      "Loss: 3.092359281182289\n",
      "**********\n",
      "epoch 35\n",
      "train_end\n",
      "trainn acc: 5483 6621 0.8281226400845794\n",
      "trainp acc: 6126 6621 0.9252378794743996\n",
      "6062596096\n",
      "test_end\n",
      "testn acc: 652 780 0.8358974358974359\n",
      "testp acc: 708 780 0.9076923076923077\n",
      "6063120384\n",
      "Loss: 4.716832742510357\n",
      "Loss: 2.4384365528821945\n",
      "**********\n",
      "epoch 36\n",
      "train_end\n",
      "trainn acc: 5500 6621 0.8306902280622263\n",
      "trainp acc: 6128 6621 0.9255399486482404\n",
      "6073344000\n",
      "test_end\n",
      "testn acc: 608 780 0.7794871794871795\n",
      "testp acc: 717 780 0.9192307692307692\n",
      "6074130432\n",
      "Loss: 4.6959575538228195\n",
      "Loss: 3.216600338220596\n",
      "**********\n",
      "epoch 37\n",
      "train_end\n",
      "trainn acc: 5505 6621 0.8314454009968283\n",
      "trainp acc: 6136 6621 0.9267482253436037\n",
      "6084354048\n",
      "test_end\n",
      "testn acc: 615 780 0.7884615384615384\n",
      "testp acc: 711 780 0.9115384615384615\n",
      "6084878336\n",
      "Loss: 4.63109839672134\n",
      "Loss: 3.1065752598643304\n",
      "**********\n",
      "epoch 38\n",
      "train_end\n",
      "trainn acc: 5538 6621 0.8364295423652016\n",
      "trainp acc: 6128 6621 0.9255399486482404\n",
      "6095101952\n",
      "test_end\n",
      "testn acc: 633 780 0.8115384615384615\n",
      "testp acc: 711 780 0.9115384615384615\n",
      "6095888384\n",
      "Loss: 4.527157703152794\n",
      "Loss: 2.8094208857417104\n",
      "**********\n",
      "epoch 39\n",
      "train_end\n",
      "trainn acc: 5522 6621 0.8340129889744752\n",
      "trainp acc: 6124 6621 0.9249358103005588\n",
      "6106112000\n",
      "test_end\n",
      "testn acc: 629 780 0.8064102564102564\n",
      "testp acc: 715 780 0.9166666666666666\n",
      "6106636288\n",
      "Loss: 4.6213230280579864\n",
      "Loss: 3.0213520167768\n",
      "**********\n",
      "epoch 40\n",
      "train_end\n",
      "trainn acc: 5580 6621 0.8427729950158587\n",
      "trainp acc: 6155 6621 0.9296178824950914\n",
      "6117122048\n",
      "test_end\n",
      "testn acc: 649 780 0.8320512820512821\n",
      "testp acc: 703 780 0.9012820512820513\n",
      "6117646336\n",
      "Loss: 4.504409125111521\n",
      "Loss: 2.5458937954902647\n",
      "**********\n",
      "epoch 41\n",
      "train_end\n",
      "trainn acc: 5513 6621 0.8326536776921916\n",
      "trainp acc: 6145 6621 0.9281075366258873\n",
      "6127869952\n",
      "test_end\n",
      "testn acc: 624 780 0.8\n",
      "testp acc: 715 780 0.9166666666666666\n",
      "6128394240\n",
      "Loss: 4.56892831424526\n",
      "Loss: 2.810447905808687\n",
      "**********\n",
      "epoch 42\n",
      "train_end\n",
      "trainn acc: 5550 6621 0.8382419574082465\n",
      "trainp acc: 6147 6621 0.9284096057997281\n",
      "6138880000\n",
      "test_end\n",
      "testn acc: 638 780 0.8179487179487179\n",
      "testp acc: 716 780 0.9179487179487179\n",
      "6139404288\n",
      "Loss: 4.533456335742022\n",
      "Loss: 2.8157405634224415\n",
      "**********\n",
      "epoch 43\n",
      "train_end\n",
      "trainn acc: 5523 6621 0.8341640235613955\n",
      "trainp acc: 6142 6621 0.9276544328651262\n",
      "6149627904\n",
      "test_end\n",
      "testn acc: 645 780 0.8269230769230769\n",
      "testp acc: 709 780 0.908974358974359\n",
      "6150152192\n",
      "Loss: 4.613545299476309\n",
      "Loss: 2.66311304718256\n",
      "**********\n",
      "epoch 44\n",
      "train_end\n",
      "trainn acc: 5519 6621 0.8335598852137139\n",
      "trainp acc: 6151 6621 0.9290137441474098\n",
      "6160670720\n",
      "test_end\n",
      "testn acc: 666 780 0.8538461538461538\n",
      "testp acc: 708 780 0.9076923076923077\n",
      "6161195008\n",
      "Loss: 4.585667116457161\n",
      "Loss: 2.2626797091960906\n",
      "**********\n",
      "epoch 45\n",
      "train_end\n",
      "trainn acc: 5504 6621 0.8312943664099078\n",
      "trainp acc: 6155 6621 0.9296178824950914\n",
      "6171418624\n",
      "test_end\n",
      "testn acc: 648 780 0.8307692307692308\n",
      "testp acc: 699 780 0.8961538461538462\n",
      "6171942912\n",
      "Loss: 4.678135888778651\n",
      "Loss: 2.5270572978258135\n",
      "**********\n",
      "epoch 46\n",
      "train_end\n",
      "trainn acc: 5544 6621 0.8373357498867241\n",
      "trainp acc: 6156 6621 0.9297689170820118\n",
      "6182428672\n",
      "test_end\n",
      "testn acc: 648 780 0.8307692307692308\n",
      "testp acc: 715 780 0.9166666666666666\n",
      "6182948864\n",
      "Loss: 4.449458233397735\n",
      "Loss: 2.7340454131364824\n",
      "**********\n",
      "epoch 47\n",
      "train_end\n",
      "trainn acc: 5564 6621 0.8403564416251321\n",
      "trainp acc: 6177 6621 0.9329406434073403\n",
      "6193172480\n",
      "test_end\n",
      "testn acc: 652 780 0.8358974358974359\n",
      "testp acc: 712 780 0.9128205128205128\n",
      "6193696768\n",
      "Loss: 4.470239749076613\n",
      "Loss: 2.4905522795021535\n",
      "**********\n",
      "epoch 48\n",
      "train_end\n",
      "trainn acc: 5538 6621 0.8364295423652016\n",
      "trainp acc: 6182 6621 0.9336958163419423\n",
      "6204178432\n",
      "test_end\n",
      "testn acc: 670 780 0.8589743589743589\n",
      "testp acc: 703 780 0.9012820512820513\n",
      "6204702720\n",
      "Loss: 4.501771030127656\n",
      "Loss: 2.2724618110060693\n",
      "**********\n",
      "epoch 49\n",
      "train_end\n",
      "trainn acc: 5570 6621 0.8412626491466546\n",
      "trainp acc: 6158 6621 0.9300709862558526\n",
      "6214926336\n",
      "test_end\n",
      "testn acc: 634 780 0.8128205128205128\n",
      "testp acc: 714 780 0.9153846153846154\n",
      "6215712768\n",
      "Loss: 4.413346810657942\n",
      "Loss: 2.783695869445801\n",
      "**********\n",
      "epoch 50\n",
      "train_end\n",
      "trainn acc: 5582 6621 0.8430750641896995\n",
      "trainp acc: 6174 6621 0.932487539646579\n",
      "6225932288\n",
      "test_end\n",
      "testn acc: 643 780 0.8243589743589743\n",
      "testp acc: 711 780 0.9115384615384615\n",
      "6226456576\n",
      "Loss: 4.388871926792684\n",
      "Loss: 2.64195432305336\n",
      "**********\n",
      "epoch 51\n",
      "train_end\n",
      "trainn acc: 5619 6621 0.8486633439057544\n",
      "trainp acc: 6174 6621 0.932487539646579\n",
      "6236938240\n",
      "test_end\n",
      "testn acc: 665 780 0.8525641025641025\n",
      "testp acc: 712 780 0.9128205128205128\n",
      "6237462528\n",
      "Loss: 4.2901414060288054\n",
      "Loss: 2.246753617525101\n",
      "**********\n",
      "epoch 52\n",
      "train_end\n",
      "trainn acc: 5591 6621 0.8444343754719831\n",
      "trainp acc: 6192 6621 0.9352061622111464\n",
      "6247686144\n",
      "test_end\n",
      "testn acc: 638 780 0.8179487179487179\n",
      "testp acc: 709 780 0.908974358974359\n",
      "6248210432\n",
      "Loss: 4.336721146650154\n",
      "Loss: 2.6495904463529585\n",
      "**********\n",
      "epoch 53\n",
      "train_end\n",
      "trainn acc: 5555 6621 0.8389971303428485\n",
      "trainp acc: 6144 6621 0.927956502038967\n",
      "6258696192\n",
      "test_end\n",
      "testn acc: 659 780 0.8448717948717949\n",
      "testp acc: 708 780 0.9076923076923077\n",
      "6259220480\n",
      "Loss: 4.472872445911481\n",
      "Loss: 2.3268901252746583\n",
      "**********\n",
      "epoch 54\n",
      "train_end\n",
      "trainn acc: 5577 6621 0.8423198912550974\n",
      "trainp acc: 6162 6621 0.9306751246035342\n",
      "6269444096\n",
      "test_end\n",
      "testn acc: 656 780 0.841025641025641\n",
      "testp acc: 712 780 0.9128205128205128\n",
      "6269968384\n",
      "Loss: 4.310357386226854\n",
      "Loss: 2.371274872124195\n",
      "**********\n",
      "epoch 55\n",
      "train_end\n",
      "trainn acc: 5623 6621 0.8492674822534361\n",
      "trainp acc: 6195 6621 0.9356592659719075\n",
      "6280454144\n",
      "test_end\n",
      "testn acc: 640 780 0.8205128205128205\n",
      "testp acc: 716 780 0.9179487179487179\n",
      "6280978432\n",
      "Loss: 4.226248781422843\n",
      "Loss: 2.7486971044540405\n",
      "**********\n",
      "epoch 56\n",
      "train_end\n",
      "trainn acc: 5610 6621 0.8473040326234708\n",
      "trainp acc: 6170 6621 0.9318834012988975\n",
      "6291202048\n",
      "test_end\n",
      "testn acc: 634 780 0.8128205128205128\n",
      "testp acc: 720 780 0.9230769230769231\n",
      "6291726336\n",
      "Loss: 4.208813877211631\n",
      "Loss: 2.9469648572802543\n",
      "**********\n",
      "epoch 57\n",
      "train_end\n",
      "trainn acc: 5584 6621 0.8433771333635403\n",
      "trainp acc: 6159 6621 0.930222020842773\n",
      "6302212096\n",
      "test_end\n",
      "testn acc: 651 780 0.8346153846153846\n",
      "testp acc: 717 780 0.9192307692307692\n",
      "6302736384\n",
      "Loss: 4.3571336260725255\n",
      "Loss: 2.430219631046057\n",
      "**********\n",
      "epoch 58\n",
      "train_end\n",
      "trainn acc: 5618 6621 0.848512309318834\n",
      "trainp acc: 6180 6621 0.9333937471681015\n",
      "6312960000\n",
      "test_end\n",
      "testn acc: 648 780 0.8307692307692308\n",
      "testp acc: 715 780 0.9166666666666666\n",
      "6313484288\n",
      "Loss: 4.306188834253504\n",
      "Loss: 2.5641303771734236\n",
      "**********\n",
      "epoch 59\n",
      "train_end\n",
      "trainn acc: 5614 6621 0.8479081709711523\n",
      "trainp acc: 6179 6621 0.9332427125811811\n",
      "6323970048\n",
      "test_end\n",
      "testn acc: 612 780 0.7846153846153846\n",
      "testp acc: 723 780 0.926923076923077\n",
      "6324494336\n",
      "Loss: 4.260531563619011\n",
      "Loss: 3.272692530453205\n",
      "**********\n",
      "epoch 60\n",
      "train_end\n",
      "trainn acc: 5644 6621 0.8524392085787645\n",
      "trainp acc: 6205 6621 0.9371696118411116\n",
      "6334717952\n",
      "test_end\n",
      "testn acc: 642 780 0.823076923076923\n",
      "testp acc: 717 780 0.9192307692307692\n",
      "6335504384\n",
      "Loss: 4.170575008298185\n",
      "Loss: 2.628627420663834\n",
      "**********\n",
      "epoch 61\n",
      "train_end\n",
      "trainn acc: 5606 6621 0.8466998942757892\n",
      "trainp acc: 6191 6621 0.9350551276242259\n",
      "6345728000\n",
      "test_end\n",
      "testn acc: 639 780 0.8192307692307692\n",
      "testp acc: 712 780 0.9128205128205128\n",
      "6346252288\n",
      "Loss: 4.252364116419068\n",
      "Loss: 2.7440747261047362\n",
      "**********\n",
      "epoch 62\n",
      "train_end\n",
      "trainn acc: 5642 6621 0.8521371394049237\n",
      "trainp acc: 6212 6621 0.9382268539495544\n",
      "6356475904\n",
      "test_end\n",
      "testn acc: 648 780 0.8307692307692308\n",
      "testp acc: 714 780 0.9153846153846154\n",
      "6357262336\n",
      "Loss: 4.129199855349759\n",
      "Loss: 2.5491190162301063\n",
      "**********\n",
      "epoch 63\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_end\n",
      "trainn acc: 5627 6621 0.8498716206011176\n",
      "trainp acc: 6184 6621 0.9339978855157831\n",
      "6367485952\n",
      "test_end\n",
      "testn acc: 632 780 0.8102564102564103\n",
      "testp acc: 704 780 0.9025641025641026\n",
      "6368010240\n",
      "Loss: 4.152944504210896\n",
      "Loss: 2.663821920156479\n",
      "**********\n",
      "epoch 64\n",
      "train_end\n",
      "trainn acc: 5634 6621 0.8509288627095605\n",
      "trainp acc: 6199 6621 0.9362634043195892\n",
      "6378496000\n",
      "test_end\n",
      "testn acc: 646 780 0.8282051282051283\n",
      "testp acc: 718 780 0.9205128205128205\n",
      "6379020288\n",
      "Loss: 4.180696082897441\n",
      "Loss: 2.633631422519684\n",
      "**********\n",
      "epoch 65\n",
      "train_end\n",
      "trainn acc: 5637 6621 0.8513819664703217\n",
      "trainp acc: 6209 6621 0.9377737501887933\n",
      "6389248000\n",
      "test_end\n",
      "testn acc: 634 780 0.8128205128205128\n",
      "testp acc: 707 780 0.9064102564102564\n",
      "6389768192\n",
      "Loss: 4.174576439404737\n",
      "Loss: 2.8642338716983797\n",
      "**********\n",
      "epoch 66\n",
      "train_end\n",
      "trainn acc: 5638 6621 0.8515330010572421\n",
      "trainp acc: 6203 6621 0.9368675426672708\n",
      "6400253952\n",
      "test_end\n",
      "testn acc: 647 780 0.8294871794871795\n",
      "testp acc: 711 780 0.9115384615384615\n",
      "6400778240\n",
      "Loss: 4.161555425388334\n",
      "Loss: 2.566300344765186\n",
      "**********\n",
      "epoch 67\n",
      "train_end\n",
      "trainn acc: 5585 6621 0.8435281679504607\n",
      "trainp acc: 6167 6621 0.9314302975381362\n",
      "6411014144\n",
      "test_end\n",
      "testn acc: 641 780 0.8217948717948718\n",
      "testp acc: 713 780 0.9141025641025641\n",
      "6411526144\n",
      "Loss: 4.256906869346395\n",
      "Loss: 2.5554457560181616\n",
      "**********\n",
      "epoch 68\n",
      "train_end\n",
      "trainn acc: 5619 6621 0.8486633439057544\n",
      "trainp acc: 6170 6621 0.9318834012988975\n",
      "6422011904\n",
      "test_end\n",
      "testn acc: 655 780 0.8397435897435898\n",
      "testp acc: 714 780 0.9153846153846154\n",
      "6422536192\n",
      "Loss: 4.251083077228997\n",
      "Loss: 2.3773826211690903\n",
      "**********\n",
      "epoch 69\n",
      "train_end\n",
      "trainn acc: 5644 6621 0.8524392085787645\n",
      "trainp acc: 6206 6621 0.937320646428032\n",
      "6432759808\n",
      "test_end\n",
      "testn acc: 635 780 0.8141025641025641\n",
      "testp acc: 713 780 0.9141025641025641\n",
      "6433284096\n",
      "Loss: 4.106627928151486\n",
      "Loss: 2.6284417057037355\n",
      "**********\n",
      "epoch 70\n",
      "train_end\n",
      "trainn acc: 5652 6621 0.8536474852741278\n",
      "trainp acc: 6209 6621 0.9377737501887933\n",
      "6443769856\n",
      "test_end\n",
      "testn acc: 649 780 0.8320512820512821\n",
      "testp acc: 711 780 0.9115384615384615\n",
      "6444294144\n",
      "Loss: 4.091529500072564\n",
      "Loss: 2.529759430587292\n",
      "**********\n",
      "epoch 71\n",
      "train_end\n",
      "trainn acc: 5669 6621 0.8562150732517747\n",
      "trainp acc: 6186 6621 0.9342999546896239\n",
      "6454517760\n",
      "test_end\n",
      "testn acc: 647 780 0.8294871794871795\n",
      "testp acc: 719 780 0.9217948717948717\n",
      "6455304192\n",
      "Loss: 4.046519045907307\n",
      "Loss: 2.4648302176594736\n",
      "**********\n",
      "epoch 72\n",
      "train_end\n",
      "trainn acc: 5654 6621 0.8539495544479686\n",
      "trainp acc: 6202 6621 0.9367165080803505\n",
      "6465527808\n",
      "test_end\n",
      "testn acc: 665 780 0.8525641025641025\n",
      "testp acc: 710 780 0.9102564102564102\n",
      "6466052096\n",
      "Loss: 4.098322537752533\n",
      "Loss: 2.2248518750071526\n",
      "**********\n",
      "epoch 73\n",
      "train_end\n",
      "trainn acc: 5647 6621 0.8528923123395258\n",
      "trainp acc: 6174 6621 0.932487539646579\n",
      "6476275712\n",
      "test_end\n",
      "testn acc: 653 780 0.8371794871794872\n",
      "testp acc: 715 780 0.9166666666666666\n",
      "6477062144\n",
      "Loss: 4.096541963855149\n",
      "Loss: 2.439576028883457\n",
      "**********\n",
      "epoch 74\n",
      "train_end\n",
      "trainn acc: 5686 6621 0.8587826612294215\n",
      "trainp acc: 6216 6621 0.9388309922972361\n",
      "6487281664\n",
      "test_end\n",
      "testn acc: 636 780 0.8153846153846154\n",
      "testp acc: 721 780 0.9243589743589744\n",
      "6487805952\n",
      "Loss: 3.9467291958136843\n",
      "Loss: 2.8529941818118094\n",
      "**********\n",
      "epoch 75\n",
      "train_end\n",
      "trainn acc: 5691 6621 0.8595378341640235\n",
      "trainp acc: 6197 6621 0.9359613351457484\n",
      "6498291712\n",
      "test_end\n",
      "testn acc: 648 780 0.8307692307692308\n",
      "testp acc: 715 780 0.9166666666666666\n",
      "6498816000\n",
      "Loss: 4.041977059152345\n",
      "Loss: 2.5006697863340377\n",
      "**********\n",
      "epoch 76\n",
      "train_end\n",
      "trainn acc: 5701 6621 0.8610481800332276\n",
      "trainp acc: 6226 6621 0.9403413381664402\n",
      "6509039616\n",
      "test_end\n",
      "testn acc: 654 780 0.8384615384615385\n",
      "testp acc: 718 780 0.9205128205128205\n",
      "6509563904\n",
      "Loss: 3.935024581910149\n",
      "Loss: 2.6119726157188414\n",
      "**********\n",
      "epoch 77\n",
      "train_end\n",
      "trainn acc: 5714 6621 0.8630116296631929\n",
      "trainp acc: 6226 6621 0.9403413381664402\n",
      "6520049664\n",
      "test_end\n",
      "testn acc: 650 780 0.8333333333333334\n",
      "testp acc: 720 780 0.9230769230769231\n",
      "6520573952\n",
      "Loss: 3.866588014568463\n",
      "Loss: 2.652853302657604\n",
      "**********\n",
      "epoch 78\n",
      "train_end\n",
      "trainn acc: 5697 6621 0.860444041685546\n",
      "trainp acc: 6205 6621 0.9371696118411116\n",
      "6530797568\n",
      "test_end\n",
      "testn acc: 662 780 0.8487179487179487\n",
      "testp acc: 703 780 0.9012820512820513\n",
      "6531321856\n",
      "Loss: 4.033088348918398\n",
      "Loss: 2.1421669733524324\n",
      "**********\n",
      "epoch 79\n",
      "train_end\n",
      "trainn acc: 5738 6621 0.8666364597492826\n",
      "trainp acc: 6243 6621 0.942908926144087\n",
      "6541807616\n",
      "test_end\n",
      "testn acc: 649 780 0.8320512820512821\n",
      "testp acc: 715 780 0.9166666666666666\n",
      "6542331904\n",
      "Loss: 3.8535257285238003\n",
      "Loss: 2.699425882101059\n",
      "**********\n",
      "epoch 80\n",
      "train_end\n",
      "trainn acc: 5694 6621 0.8599909379247848\n",
      "trainp acc: 6202 6621 0.9367165080803505\n",
      "6552555520\n",
      "test_end\n",
      "testn acc: 682 780 0.8743589743589744\n",
      "testp acc: 705 780 0.9038461538461539\n",
      "6553079808\n",
      "Loss: 3.979236285462889\n",
      "Loss: 2.1213447871804236\n",
      "**********\n",
      "epoch 81\n",
      "train_end\n",
      "trainn acc: 5728 6621 0.8651261138800785\n",
      "trainp acc: 6214 6621 0.9385289231233953\n",
      "6563565568\n",
      "test_end\n",
      "testn acc: 642 780 0.823076923076923\n",
      "testp acc: 725 780 0.9294871794871795\n",
      "6564089856\n",
      "Loss: 3.866291849316897\n",
      "Loss: 2.765933291912079\n",
      "**********\n",
      "epoch 82\n",
      "train_end\n",
      "trainn acc: 5699 6621 0.8607461108593868\n",
      "trainp acc: 6215 6621 0.9386799577103156\n",
      "6574313472\n",
      "test_end\n",
      "testn acc: 659 780 0.8448717948717949\n",
      "testp acc: 716 780 0.9179487179487179\n",
      "6574837760\n",
      "Loss: 3.9446469777621735\n",
      "Loss: 2.430710504055023\n",
      "**********\n",
      "epoch 83\n",
      "train_end\n",
      "trainn acc: 5719 6621 0.8637668025977949\n",
      "trainp acc: 6226 6621 0.9403413381664402\n",
      "6585323520\n",
      "test_end\n",
      "testn acc: 645 780 0.8269230769230769\n",
      "testp acc: 717 780 0.9192307692307692\n",
      "6585847808\n",
      "Loss: 3.8402891414506093\n",
      "Loss: 2.717055595219135\n",
      "**********\n",
      "epoch 84\n",
      "train_end\n",
      "trainn acc: 5773 6621 0.8719226702914967\n",
      "trainp acc: 6260 6621 0.9454765141217338\n",
      "6596071424\n",
      "test_end\n",
      "testn acc: 658 780 0.8435897435897436\n",
      "testp acc: 720 780 0.9230769230769231\n",
      "6596857856\n",
      "Loss: 3.699251982618154\n",
      "Loss: 2.3819554778933525\n",
      "**********\n",
      "epoch 85\n",
      "train_end\n",
      "trainn acc: 5701 6621 0.8610481800332276\n",
      "trainp acc: 6208 6621 0.9376227156018728\n",
      "6607081472\n",
      "test_end\n",
      "testn acc: 651 780 0.8346153846153846\n",
      "testp acc: 722 780 0.9256410256410257\n",
      "6607605760\n",
      "Loss: 3.9321600969764\n",
      "Loss: 2.648771605491638\n",
      "**********\n",
      "epoch 86\n",
      "train_end\n",
      "trainn acc: 5737 6621 0.8664854251623622\n",
      "trainp acc: 6233 6621 0.941398580274883\n",
      "6618091520\n",
      "test_end\n",
      "testn acc: 659 780 0.8448717948717949\n",
      "testp acc: 723 780 0.926923076923077\n",
      "6618615808\n",
      "Loss: 3.8027451509278825\n",
      "Loss: 2.4632556974887847\n",
      "**********\n",
      "epoch 87\n",
      "train_end\n",
      "trainn acc: 5757 6621 0.8695061169007703\n",
      "trainp acc: 6237 6621 0.9420027186225646\n",
      "6628839424\n",
      "test_end\n",
      "testn acc: 651 780 0.8346153846153846\n",
      "testp acc: 723 780 0.926923076923077\n",
      "6629363712\n",
      "Loss: 3.7587186728758817\n",
      "Loss: 2.508786410689354\n",
      "**********\n",
      "epoch 88\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import sys\n",
    "import pickle\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import codecs\n",
    "import numpy as np\n",
    "import importlib\n",
    "import gensim\n",
    "import random\n",
    "import psutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.utils.data\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "import torch.nn.init as init\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "torch.cuda.set_device(1)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "args = {\n",
    "    \"optimizer\": 'adam',\n",
    "    \"loss_function\":'binary_crossentropy',\n",
    "    \"initializer\": 'xavier_uniform_',\n",
    "    \"num_epoch\":500,\n",
    "    \"batch_size\":1,\n",
    "    \"embed_dim\":60,\n",
    "    \"num_lstm\":60,\n",
    "    \"hidden_dim\":240,\n",
    "    \"max_seq_len\":230,\n",
    "    \"MAX_NB_WORDS\":200000,\n",
    "    \"VALIDATION_SPLIT\":0.1,\n",
    "    \"rate_drop_lstm\":0.5,\n",
    "    \"rate_drop_dense\":0.5,\n",
    "    \"act\":'relu',\n",
    "    \"re_weight\":True,\n",
    "    \"init_weight_value\" :6.0\n",
    "}\n",
    "\n",
    "vocb=[]\n",
    "for i in range(len(content)):\n",
    "    for j in range(len(content[i])):\n",
    "        vocb+=content[i][j].split(\" \")\n",
    "vocb=set(vocb)\n",
    "\n",
    "word_to_idx = {word: i for i, word in enumerate(vocb)}\n",
    "idx_to_word = {word_to_idx[word]: word for word in word_to_idx}\n",
    "\n",
    "EMBEDDING_FILE='/media/sysu502/data/duxin/10G训练好的词向量/60维/Word60.model'\n",
    "STAMP = 'lstm_%d_%d_%.2f_%.2f' % (args['num_lstm'], args['hidden_dim'], args['rate_drop_lstm'], \\\n",
    "                                                args['rate_drop_dense'])\n",
    "\n",
    "word2vec = gensim.models.deprecated.word2vec.Word2Vec.load(EMBEDDING_FILE)\n",
    "nb_words = min(args['MAX_NB_WORDS'], len(vocb)) + 1\n",
    "embedding_matrix = np.zeros((nb_words, args['embed_dim']))\n",
    "\n",
    "for word, i in word_to_idx.items():\n",
    "    if word in word2vec.wv.vocab:\n",
    "        embedding_matrix[i] = word2vec.wv.word_vec(word)\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "\n",
    "class ScheduledOptim(object):\n",
    "    \"\"\"A wrapper class for learning rate scheduling\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        self.lr = self.optimizer.param_groups[0]['lr']\n",
    "        self.current_steps = 0\n",
    "\n",
    "    def step(self):\n",
    "        \"Step by the inner optimizer\"\n",
    "        self.current_steps += 1\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def zero_grad(self):\n",
    "        \"Zero out the gradients by the inner optimizer\"\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "    def lr_multi(self, multi):\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] *= multi\n",
    "        self.lr = self.optimizer.param_groups[0]['lr']\n",
    "\n",
    "    def set_learning_rate(self, lr):\n",
    "        self.lr = lr\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "    @property\n",
    "    def learning_rate(self):\n",
    "        return self.lr\n",
    "\n",
    "def generate_mask_2(values, sent_sizes):\n",
    "    mask_matrix = np.zeros((len(sent_sizes), max(sent_sizes), values.size(2)))\n",
    "    for i in range(len(sent_sizes)):\n",
    "        mask_matrix[i][:sent_sizes[i]][:]=1\n",
    "    if torch.cuda.is_available():\n",
    "        mask_matrix = torch.Tensor(mask_matrix).cuda()\n",
    "    else:\n",
    "        mask_matrix = torch.Tensor(mask_matrix)\n",
    "    return values*Variable(mask_matrix)\n",
    "\n",
    "def generate_mask(lsent_sizes, rsent_sizes):\n",
    "    mask_matrix=np.zeros((len(lsent_sizes),max(lsent_sizes),max(rsent_sizes)))\n",
    "    for i in range(len(lsent_sizes)):\n",
    "        mask_matrix[i][:lsent_sizes[i]][:rsent_sizes[i]]=1\n",
    "    if torch.cuda.is_available():\n",
    "        mask_matrix = torch.Tensor(mask_matrix).cuda()\n",
    "    else:\n",
    "        mask_matrix = torch.Tensor(mask_matrix)\n",
    "    return Variable(mask_matrix)\n",
    "\n",
    "class DecAtt(nn.Module):\n",
    "    def __init__(self, num_units, num_classes, vocab_size, embedding_size,\n",
    "                 pretrained_emb, word_att,training=True, project_input=True,\n",
    "                 use_intra_attention=False, distance_biases=10, max_sentence_length=30):\n",
    "        super(DecAtt, self).__init__()\n",
    "        self.vocab_size=vocab_size\n",
    "        self.num_units = num_units\n",
    "        self.num_classes = num_classes\n",
    "        self.project_input = project_input\n",
    "        self.embedding_size=embedding_size\n",
    "        self.distance_biases=distance_biases\n",
    "        self.intra_attention=False\n",
    "        self.max_sentence_length=max_sentence_length\n",
    "        self.pretrained_emb=pretrained_emb\n",
    "\n",
    "        self.bias_embedding=nn.Embedding(max_sentence_length,1)\n",
    "        self.word_embedding=nn.Embedding.from_pretrained(torch.tensor(pretrained_emb, dtype=torch.float))\n",
    "        self.linear_layer_project = nn.Linear(embedding_size, num_units, bias=False)\n",
    "\n",
    "        self.linear_layer_wordatt1 = nn.Sequential(nn.Dropout(p=0.2), nn.Linear(num_units, num_units), nn.ReLU(inplace=True))\n",
    "        self.linear_layer_wordatt2 = nn.Sequential(nn.Dropout(p=0.2), nn.Linear(num_units, num_units), nn.ReLU(inplace=True))\n",
    "        \n",
    "        self.linear_layer_attend = nn.Sequential(nn.Dropout(p=0.2), nn.Linear(num_units, num_units), nn.ReLU(inplace=True),\n",
    "                                                 nn.Dropout(p=0.2), nn.Linear(num_units, num_units), nn.ReLU(inplace=True))\n",
    "\n",
    "        self.linear_layer_compare = nn.Sequential(nn.Dropout(p=0.2), nn.Linear(num_units*2, num_units), nn.ReLU(inplace=True),\n",
    "                                                  nn.Dropout(p=0.2), nn.Linear(num_units, num_units), nn.ReLU(inplace=True))\n",
    "\n",
    "        self.linear_layer_aggregate = nn.Sequential(nn.Dropout(p=0.2), nn.Linear(num_units*4, num_units), nn.ReLU(inplace=True),\n",
    "                                                    nn.Dropout(p=0.2), nn.Linear(num_units, num_units), nn.ReLU(inplace=True),\n",
    "                                                    nn.Linear(num_units, num_classes))\n",
    "        \n",
    "        nn.MaxPool2d\n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        self.linear_layer_project.weight.data.normal_(0, 0.01)\n",
    "        self.linear_layer_wordatt1[1].weight.data.normal_(0, 0.01)\n",
    "        self.linear_layer_wordatt1[1].bias.data.fill_(0)\n",
    "        self.linear_layer_wordatt2[1].weight.data.normal_(0, 0.01)\n",
    "        self.linear_layer_wordatt2[1].bias.data.fill_(0)\n",
    "        self.linear_layer_attend[1].weight.data.normal_(0, 0.01)\n",
    "        self.linear_layer_attend[1].bias.data.fill_(0)\n",
    "        self.linear_layer_attend[4].weight.data.normal_(0, 0.01)\n",
    "        self.linear_layer_attend[4].bias.data.fill_(0)\n",
    "        self.linear_layer_compare[1].weight.data.normal_(0, 0.01)\n",
    "        self.linear_layer_compare[1].bias.data.fill_(0)\n",
    "        self.linear_layer_compare[4].weight.data.normal_(0, 0.01)\n",
    "        self.linear_layer_compare[4].bias.data.fill_(0)\n",
    "        self.linear_layer_aggregate[1].weight.data.normal_(0, 0.01)\n",
    "        self.linear_layer_aggregate[1].bias.data.fill_(0)\n",
    "        self.linear_layer_aggregate[4].weight.data.normal_(0, 0.01)\n",
    "        self.linear_layer_aggregate[4].bias.data.fill_(0)\n",
    "\n",
    "    def attention_softmax3d(self,raw_attentions):\n",
    "        reshaped_attentions = raw_attentions.view(-1, raw_attentions.size(2))\n",
    "        out=nn.functional.softmax(reshaped_attentions, dim=1)\n",
    "        return out.view(raw_attentions.size(0),raw_attentions.size(1),raw_attentions.size(2))\n",
    "    \n",
    "    def attention_softmax2d(self,raw_attentions):\n",
    "        reshaped_attentions = raw_attentions\n",
    "        out=nn.functional.softmax(reshaped_attentions, dim=0)\n",
    "        return out\n",
    "\n",
    "    def _transformation_input(self,embed_sent):\n",
    "        embed_sent=self.word_embedding(embed_sent)\n",
    "        embed_sent=self.linear_layer_project(embed_sent)\n",
    "        result=embed_sent\n",
    "        if self.intra_attention:\n",
    "            f_intra = self.linear_layer_intra(embed_sent)\n",
    "            f_intra_t = torch.transpose(f_intra, 1, 2)\n",
    "            raw_attentions = torch.matmul(f_intra, f_intra_t)\n",
    "            time_steps=embed_sent.size(1)\n",
    "            r = torch.arange(0, time_steps)\n",
    "            #从start到end的一组序列值，以1为间隔，不包括两端\n",
    "            r_matrix=r.view(1,-1).expand(time_steps,time_steps)\n",
    "            raw_index=r_matrix-r.view(-1,1)\n",
    "            clipped_index=torch.clamp(raw_index,0,self.distance_biases-1)\n",
    "            clipped_index=Variable(clipped_index.long())\n",
    "            if torch.cuda.is_available():\n",
    "                clipped_index=clipped_index.cuda()\n",
    "            bias=self.bias_embedding(clipped_index)\n",
    "            bias=torch.squeeze(bias)\n",
    "            raw_attentions += bias\n",
    "            attentions = self.attention_softmax3d(raw_attentions)\n",
    "            attended = torch.matmul(attentions, embed_sent)\n",
    "            result =torch.cat([embed_sent,attended],2)\n",
    "        return result\n",
    "\n",
    "    def attend(self,sent1,sent2):\n",
    "        repr1=self.linear_layer_attend(sent1)\n",
    "        repr2=self.linear_layer_attend(sent2)\n",
    "        repr2=torch.transpose(repr2,1,2)\n",
    "        raw_attentions = torch.matmul(repr1, repr2)\n",
    "\n",
    "        att_sent1 = self.attention_softmax3d(raw_attentions)\n",
    "        beta=torch.matmul(att_sent1,sent2) #input2_soft\n",
    "\n",
    "        raw_attentions_t=torch.transpose(raw_attentions,1,2).contiguous()\n",
    "        att_sent2 = self.attention_softmax3d(raw_attentions_t)\n",
    "        alpha=torch.matmul(att_sent2,sent1) #input1_soft\n",
    "\n",
    "        return alpha, beta\n",
    "\n",
    "    def compare(self,sentence,soft_alignment):\n",
    "        sent_alignment=torch.cat([sentence, soft_alignment],2)\n",
    "        out = self.linear_layer_compare(sent_alignment)\n",
    "        return out\n",
    "\n",
    "    def aggregate(self,v1, v2):\n",
    "        v1_sum=torch.sum(v1,1)\n",
    "        v2_sum=torch.sum(v2,1)\n",
    "        return torch.cat([v1_sum,v2_sum],1)\n",
    "\n",
    "    def forward(self,train_1,train_2,train1_att,train2_att):\n",
    "        maxp_dvector=[]\n",
    "#         len1=0\n",
    "#         if len(train_1)<len(train_2):\n",
    "#             len1=len(train_1)\n",
    "#         else:\n",
    "#             len1=len(train_2)\n",
    "        for i in range(len(train_2)):\n",
    "            j1=len(train_1[0])-1\n",
    "            while 1:\n",
    "                if train_1[0,j1]==0:\n",
    "                    j1-=1\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "            j2=len(train_2[i])-1\n",
    "            while 1:\n",
    "                if train_2[i,j2]==0:\n",
    "                    j2-=1\n",
    "                else:\n",
    "                    break\n",
    "        \n",
    "            sent1=self._transformation_input(train_1[0,:j1+1].unsqueeze(0))\n",
    "            sent2=self._transformation_input(train_2[i,:j2+1].unsqueeze(0))\n",
    "            alpha, beta = self.attend(sent1, sent2)\n",
    "            v1=self.compare(sent1,beta)\n",
    "            v2=self.compare(sent2,alpha)\n",
    "            logits=self.aggregate(v1,v2)\n",
    "            if i==0:\n",
    "                maxp_dvector=logits\n",
    "            else:\n",
    "                maxp_dvector=torch.cat([maxp_dvector,logits],0)\n",
    "#         maxp_dvector=maxp_dvector.unsqueeze(0)\n",
    "        maxp_dvector=torch.mean(maxp_dvector,0)\n",
    "        sent1=self._transformation_input(train1_att.unsqueeze(0))\n",
    "        sent2=self._transformation_input(train2_att.unsqueeze(0))\n",
    "        alpha, beta = self.attend(sent1, sent2)\n",
    "        v1=self.compare(sent1,beta)\n",
    "        v2=self.compare(sent2,alpha)\n",
    "        uvector=self.aggregate(v1,v2)\n",
    "        concat_vector=torch.cat([maxp_dvector.unsqueeze(0),uvector],1)\n",
    "        logits=self.linear_layer_aggregate(concat_vector)\n",
    "            \n",
    "        return logits\n",
    "\n",
    "train_1=[]\n",
    "train_2=[]\n",
    "test_1=[]\n",
    "test_2=[]\n",
    "train1_att=[]\n",
    "test1_att=[]\n",
    "train2_att=[]\n",
    "test2_att=[]\n",
    "maxl=0\n",
    "for i in range(len(train1)):\n",
    "    train_1.append([])\n",
    "    for k in range(len(train1[i])):\n",
    "        x=train1[i][k].split(\" \")\n",
    "        if maxl<len(x):\n",
    "            maxl=len(x)\n",
    "        x1= [word_to_idx[j] for j in x]\n",
    "        train_1[-1].append(x1)\n",
    "for i in range(len(train2)):\n",
    "    train_2.append([])\n",
    "    for k in range(len(train2[i])):\n",
    "        x=train2[i][k].split(\" \")\n",
    "        if maxl<len(x):\n",
    "            maxl=len(x)\n",
    "        x1= [word_to_idx[j] for j in x]\n",
    "        train_2[-1].append(x1)\n",
    "for i in range(len(test1)):\n",
    "    test_1.append([])\n",
    "    for k in range(len(test1[i])):\n",
    "        x=test1[i][k].split(\" \")\n",
    "        if maxl<len(x):\n",
    "            maxl=len(x)\n",
    "        x1= [word_to_idx[j] for j in x]\n",
    "        test_1[-1].append(x1)\n",
    "for i in range(len(test2)):\n",
    "    test_2.append([])\n",
    "    for k in range(len(test2[i])):\n",
    "        x=test2[i][k].split(\" \")\n",
    "        if maxl<len(x):\n",
    "            maxl=len(x)\n",
    "        x1= [word_to_idx[j] for j in x]\n",
    "        test_2[-1].append(x1)\n",
    "\n",
    "for i in range(len(wordstoprank1001)):\n",
    "    train1_att.append([])\n",
    "    for j in range(len(wordstoprank1001[i])):\n",
    "        x= [word_to_idx[k] for k in wordstoprank1001[i][j]]\n",
    "        train1_att[-1].append(x)\n",
    "for i in range(len(wordstoptrank1001)):\n",
    "    test1_att.append([])\n",
    "    for j in range(len(wordstoptrank1001[i])):\n",
    "        x= [word_to_idx[k] for k in wordstoptrank1001[i][j]]\n",
    "        test1_att[-1].append(x)\n",
    "for i in range(len(wordstoprank1002)):\n",
    "    train2_att.append([])\n",
    "    for j in range(len(wordstoprank1002[i])):\n",
    "        x= [word_to_idx[k] for k in wordstoprank1002[i][j]]\n",
    "        train2_att[-1].append(x)\n",
    "for i in range(len(wordstoptrank1002)):\n",
    "    test2_att.append([])\n",
    "    for j in range(len(wordstoptrank1002[i])):\n",
    "        x= [word_to_idx[k] for k in wordstoptrank1002[i][j]]\n",
    "        test2_att[-1].append(x)\n",
    "    \n",
    "args['max_seq_len']=maxl\n",
    "\n",
    "print(maxl)\n",
    "\n",
    "for i in range(len(train_1)):\n",
    "    for j in range(len(train_1[i])):\n",
    "        while 1:\n",
    "            if len(train_1[i][j])<args['max_seq_len']:\n",
    "                train_1[i][j].append(0)\n",
    "            else:\n",
    "                break\n",
    "    for j in range(len(train_2[i])):\n",
    "        while 1:\n",
    "            if len(train_2[i][j])<args['max_seq_len']:\n",
    "                train_2[i][j].append(0)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "for i in range(len(test_1)):\n",
    "    for j in range(len(test_1[i])):\n",
    "        while 1:\n",
    "            if len(test_1[i][j])<args['max_seq_len']:\n",
    "                test_1[i][j].append(0)\n",
    "            else:\n",
    "                break\n",
    "    for j in range(len(test_2[i])):\n",
    "        while 1:\n",
    "            if len(test_2[i][j])<args['max_seq_len']:\n",
    "                test_2[i][j].append(0)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "# train_1=Variable(torch.LongTensor(train_1))\n",
    "# train_2=Variable(torch.LongTensor(train_2))\n",
    "# test_1=Variable(torch.LongTensor(test_1))\n",
    "# test_2=Variable(torch.LongTensor(test_2))\n",
    "# train1_att=Variable(torch.LongTensor(train1_att))\n",
    "# train2_att=Variable(torch.LongTensor(train2_att))\n",
    "# test1_att=Variable(torch.LongTensor(test1_att))\n",
    "# test2_att=Variable(torch.LongTensor(test2_att))\n",
    "# if torch.cuda.is_available():\n",
    "#     train_1 = train_1.cuda()\n",
    "#     train_2 = train_2.cuda()\n",
    "#     test_1=test_1.cuda()\n",
    "#     test_2=test_2.cuda()\n",
    "#     train1_att=train1_att.cuda()\n",
    "#     train2_att=train2_att.cuda()\n",
    "#     test1_att=test1_att.cuda()\n",
    "#     test2_att=test2_att.cuda()\n",
    "\n",
    "model = DecAtt(200,2,nb_words,args[\"embed_dim\"],embedding_matrix,100)\n",
    "# num_units, num_classes, vocab_size, embedding_size,pretrained_emb\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad,model.parameters()),lr=0.0005)\n",
    "# optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad,model.parameters()), lr = 0.0005,momentum=0.9,nesterov=True)\n",
    "optimizer=torch.optim.Adagrad(filter(lambda p: p.requires_grad,model.parameters()), lr=0.0012, weight_decay=0)\n",
    "optimizer = ScheduledOptim(optimizer)\n",
    "lr_mult = (1 / 1e-5) ** (1 / 100)\n",
    "lr = []\n",
    "losses = []\n",
    "best_loss = 1e9\n",
    "pmacc=0\n",
    "macc=0\n",
    "decaylr=0.7\n",
    "\n",
    "for epoch in range(args['num_epoch']):\n",
    "    model.train()\n",
    "    print('*' * 10)\n",
    "    print('epoch {}'.format(epoch + 1))\n",
    "    running_loss = 0\n",
    "    yp=0\n",
    "    yn=0\n",
    "    tp=0\n",
    "    tn=0\n",
    "    for i in range(len(train_1)):\n",
    "        \n",
    "        len1=0\n",
    "        if len(train_1[i])<len(train_2[i]):\n",
    "            len1=len(train_1[i])\n",
    "        else:\n",
    "            len1=len(train_2[i])\n",
    "        \n",
    "        if len1/text_group<1:\n",
    "            continue\n",
    "        for j in range(math.floor(len1/text_group)):\n",
    "            out=model(Variable(torch.LongTensor(train_1[i][j*text_group:(j+1)*text_group])).cuda(),Variable(torch.LongTensor(train_2[i][j*text_group:(j+1)*text_group])).cuda(),Variable(torch.LongTensor(train1_att[i][j])).cuda(),Variable(torch.LongTensor(train2_att[i][j])).cuda())\n",
    "        \n",
    "            loss = criterion(out, torch.LongTensor([1]).cuda())\n",
    "            running_loss += loss.data.item()\n",
    "            pred_y = torch.max(out.cpu(), 1)[1].data.numpy().squeeze()\n",
    "            yp+=1\n",
    "            if pred_y.item()==1:\n",
    "                tp+=1\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            for i1 in range(1):\n",
    "                while 1:\n",
    "                    index1=random.randint(0,len(train_1)-1)\n",
    "                    if index1!=i and len(train_2[index1])>text_group:\n",
    "                        break\n",
    "\n",
    "                index2=math.floor(random.randint(0,len(train_2[index1])-text_group-1)/text_group)\n",
    "                out=model(Variable(torch.LongTensor(train_1[i][j*text_group:(j+1)*text_group])).cuda(),Variable(torch.LongTensor(train_2[index1][index2*text_group:(index2+1)*text_group])).cuda(),Variable(torch.LongTensor(train1_att[i][j])).cuda(),Variable(torch.LongTensor(train2_att[index1][index2])).cuda())\n",
    "\n",
    "                loss = criterion(out, torch.LongTensor([0]).cuda())\n",
    "                running_loss += loss.data.item()\n",
    "                pred_y = torch.max(out.cpu(), 1)[1].data.numpy().squeeze()\n",
    "                yn+=1\n",
    "                if pred_y.item()==0:\n",
    "                    tn+=1\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    print(\"train_end\")\n",
    "    print('trainn acc:', tn, yn, tn/yn)\n",
    "    print('trainp acc:', tp, yp, tp/yp)\n",
    "    print(psutil.Process(os.getpid()).memory_info().rss)\n",
    "#     if (tn/yn+tp/yp)/2-decaylr>0:\n",
    "#         decaylr+=0.01\n",
    "#         optimizer.set_learning_rate(optimizer.learning_rate/lr_mult)\n",
    "    model.eval()\n",
    "    yp=0\n",
    "    yn=0\n",
    "    tp=0\n",
    "    tn=0\n",
    "    pre_i=0\n",
    "    running_loss1 = 0\n",
    "    for i in range(len(test_1)):\n",
    "        len1=0\n",
    "        if len(test_1[i])<len(test_2[i]):\n",
    "            len1=len(test_1[i])\n",
    "        else:\n",
    "            len1=len(test_2[i])\n",
    "            \n",
    "        if len1/text_group<1:\n",
    "            continue\n",
    "        for j in range(math.floor(len1/text_group)):\n",
    "            out=model(Variable(torch.LongTensor(test_1[i][j*text_group:(j+1)*text_group])).cuda(),Variable(torch.LongTensor(test_2[i][j*text_group:(j+1)*text_group])).cuda(),Variable(torch.LongTensor(test1_att[i][j])).cuda(),Variable(torch.LongTensor(test2_att[i][j])).cuda())\n",
    "\n",
    "            loss = criterion(out, torch.LongTensor([1]).cuda())\n",
    "            running_loss += loss.data.item()\n",
    "            pred_y = torch.max(out.cpu(), 1)[1].data.numpy().squeeze()\n",
    "            yp+=1\n",
    "            if pred_y.item()==1:\n",
    "                tp+=1\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            for i1 in range(1):\n",
    "                while 1:\n",
    "                    index1=random.randint(0,len(test_1)-1)\n",
    "                    if index1!=i and len(test_2[index1])>text_group:\n",
    "                        break\n",
    "\n",
    "                index2=math.floor(random.randint(0,len(test_2[index1])-text_group-1)/text_group)\n",
    "                out=model(Variable(torch.LongTensor(test_1[i][j*text_group:(j+1)*text_group])).cuda(),Variable(torch.LongTensor(test_2[index1][index2*text_group:(index2+1)*text_group])).cuda(),Variable(torch.LongTensor(test1_att[i][j])).cuda(),Variable(torch.LongTensor(test2_att[index1][index2])).cuda())\n",
    "\n",
    "                loss = criterion(out, torch.LongTensor([0]).cuda())\n",
    "                running_loss1 += loss.data.item()\n",
    "                pred_y = torch.max(out.cpu(), 1)[1].data.numpy().squeeze()\n",
    "                yn+=1\n",
    "                if pred_y.item()==0:\n",
    "                    tn+=1\n",
    "                optimizer.zero_grad()\n",
    "    print(\"test_end\")\n",
    "    print('testn acc:', tn, yn, tn/yn)\n",
    "    print('testp acc:', tp, yp, tp/yp)\n",
    "    print(psutil.Process(os.getpid()).memory_info().rss)\n",
    "    \n",
    "    \n",
    "    print('Loss: {}'.format(running_loss / len(train_1)))\n",
    "    print('Loss: {}'.format(running_loss1 / len(test_1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_11=Variable(torch.LongTensor(train_1[i][j*10:(j+1)*10])).cuda()\n",
    "train_22=Variable(torch.LongTensor(train_2[i][j*10:(j+1)*10])).cuda()\n",
    "train1_att1=Variable(torch.LongTensor(train1_att[i][j*10:(j+1)*10])).cuda()\n",
    "train2_att1=Variable(torch.LongTensor(train2_att[i][j*10:(j+1)*10])).cuda()\n",
    "\n",
    "maxp_dvector=[]\n",
    "for i1 in range(len(train_22)):\n",
    "    j1=len(train_11[0])-1\n",
    "    while 1:\n",
    "        if train_11[0,j1]==0:\n",
    "            j1-=1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    j2=len(train_22[i1])-1\n",
    "    while 1:\n",
    "        if train_22[i1,j2]==0:\n",
    "            j2-=1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    sent1=model._transformation_input(train_11[0,:j1+1].unsqueeze(0))\n",
    "    sent2=model._transformation_input(train_22[i1,:j2+1].unsqueeze(0))\n",
    "    alpha, beta = model.attend(sent1, sent2)\n",
    "    v1=model.compare(sent1,beta)\n",
    "    v2=model.compare(sent2,alpha)\n",
    "    logits=model.aggregate(v1,v2)\n",
    "    if i1==0:\n",
    "        maxp_dvector=logits\n",
    "    else:\n",
    "        maxp_dvector=torch.cat([maxp_dvector,logits],0)\n",
    "#         maxp_dvector=maxp_dvector.unsqueeze(0)\n",
    "maxp_dvector=torch.max(maxp_dvector,0)[0]\n",
    "sent1=model._transformation_input(train1_att1.unsqueeze(0))\n",
    "sent2=model._transformation_input(train2_att1.unsqueeze(0))\n",
    "alpha, beta = model.attend(sent1, sent2)\n",
    "v1=model.compare(sent1,beta)\n",
    "v2=model.compare(sent2,alpha)\n",
    "uvector=model.aggregate(v1,v2)\n",
    "concat_vector=torch.cat([maxp_dvector.unsqueeze(0),uvector],1)\n",
    "logits=model.linear_layer_aggregate(concat_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "input1 = open('/home/sysu502/Public/duxin/text4/douban_broadcaste44.txt', 'r',encoding=\"utf-8\")\n",
    "content=eval(input1.read())\n",
    "input1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sysu502/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:41: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.9780640006065369 1\n",
      "0.9780640006065369 1\n",
      "1\n",
      "0.9747328162193298 1\n",
      "0.9747328162193298 1\n",
      "2\n",
      "0.9214175939559937 1\n",
      "0.8920028805732727 1\n",
      "3\n",
      "0.966833770275116 1\n",
      "0.966833770275116 1\n",
      "4\n",
      "0.9799709916114807 1\n",
      "0.9799709916114807 1\n",
      "5\n",
      "0.9999985694885254 1\n",
      "0.9999985694885254 1\n",
      "6\n",
      "0.9697672128677368 1\n",
      "0.9293823838233948 1\n",
      "7\n",
      "0.9999342560768127 1\n",
      "0.9999342560768127 1\n",
      "8\n",
      "0.9707826375961304 1\n",
      "0.9422178268432617 1\n",
      "9\n",
      "0.9568617343902588 1\n",
      "0.9055628776550293 1\n",
      "10\n",
      "0.9990440011024475 1\n",
      "0.9990440011024475 1\n",
      "11\n",
      "0.9628667235374451 1\n",
      "0.8976401090621948 1\n",
      "12\n",
      "0.9796913266181946 1\n",
      "0.9796913266181946 1\n",
      "13\n",
      "0.9425537586212158 1\n",
      "0.8762763142585754 1\n",
      "14\n",
      "0.9806063771247864 1\n",
      "0.7017678618431091 1\n",
      "15\n",
      "0.9528141617774963 1\n",
      "0.9528141617774963 1\n",
      "16\n",
      "0.9743732810020447 1\n",
      "0.9479549527168274 1\n",
      "17\n",
      "0.922212541103363 1\n",
      "0.922212541103363 1\n",
      "18\n",
      "0.9937052130699158 1\n",
      "0.9937052130699158 1\n",
      "19\n",
      "0.9485782384872437 1\n",
      "0.4425576329231262 0\n",
      "20\n",
      "0.950064480304718 1\n",
      "0.6483310461044312 1\n",
      "21\n",
      "0.920221745967865 1\n",
      "0.8780270218849182 1\n",
      "22\n",
      "0.9606241583824158 1\n",
      "0.955696702003479 1\n",
      "23\n",
      "0.9285711646080017 1\n",
      "0.9285711646080017 1\n",
      "24\n",
      "0.9569984078407288 1\n",
      "0.9569984078407288 1\n",
      "25\n",
      "0.9511764049530029 1\n",
      "0.937959611415863 1\n",
      "26\n",
      "0.9670354127883911 1\n",
      "0.9482518434524536 1\n",
      "27\n",
      "0.9634546637535095 1\n",
      "0.9634546637535095 1\n",
      "28\n",
      "0.9936283230781555 1\n",
      "0.9936283230781555 1\n",
      "29\n",
      "0.8993905186653137 1\n",
      "0.7604289054870605 1\n",
      "30\n",
      "0.9684277772903442 1\n",
      "0.6323591470718384 1\n",
      "31\n",
      "0.9622728228569031 1\n",
      "0.9622728228569031 1\n",
      "32\n",
      "0.9895966053009033 1\n",
      "0.9747544527053833 1\n",
      "33\n",
      "0.9740172028541565 1\n",
      "0.9740172028541565 1\n",
      "34\n",
      "0.9729861617088318 1\n",
      "0.9590379595756531 1\n",
      "35\n",
      "0.9397046566009521 1\n",
      "0.9319895505905151 1\n",
      "36\n",
      "0.932620108127594 1\n",
      "0.7981356978416443 1\n",
      "37\n",
      "0.987644612789154 1\n",
      "0.987644612789154 1\n",
      "38\n",
      "0.9769505858421326 1\n",
      "0.9769505858421326 1\n",
      "39\n",
      "0.9325698018074036 1\n",
      "0.8694056272506714 1\n",
      "40\n",
      "0.9608964323997498 1\n",
      "0.9306514859199524 1\n",
      "41\n",
      "0.9861407279968262 1\n",
      "0.9861407279968262 1\n",
      "42\n",
      "0.9491420984268188 1\n",
      "0.5521054267883301 1\n",
      "43\n",
      "0.9432483911514282 1\n",
      "0.8353372812271118 1\n",
      "44\n",
      "0.9908965826034546 1\n",
      "0.9908965826034546 1\n",
      "45\n",
      "0.7687569856643677 1\n",
      "0.7687569856643677 1\n",
      "46\n",
      "0.9735785126686096 1\n",
      "0.9735785126686096 1\n",
      "47\n",
      "0.8639545440673828 1\n",
      "0.8048410415649414 1\n",
      "48\n",
      "0.9714727997779846 1\n",
      "0.9714727997779846 1\n",
      "49\n",
      "0.9962603449821472 1\n",
      "0.9962603449821472 1\n",
      "50\n",
      "0.9474737048149109 1\n",
      "0.9399294853210449 1\n",
      "51\n",
      "0.973125159740448 1\n",
      "0.973125159740448 1\n",
      "52\n",
      "0.9664728045463562 1\n",
      "0.9664728045463562 1\n",
      "53\n",
      "0.9479423761367798 1\n",
      "0.755815863609314 1\n",
      "54\n",
      "0.9708002209663391 1\n",
      "0.9690847992897034 1\n",
      "55\n",
      "0.951347291469574 1\n",
      "0.951347291469574 1\n",
      "56\n",
      "0.9612379670143127 1\n",
      "0.7144787311553955 1\n",
      "57\n",
      "0.9125682711601257 1\n",
      "0.8840330243110657 1\n",
      "58\n",
      "0.9946510791778564 1\n",
      "0.98512202501297 1\n",
      "59\n",
      "0.9518871307373047 1\n",
      "0.9518871307373047 1\n",
      "60\n",
      "0.9434451460838318 1\n",
      "0.9434451460838318 1\n",
      "61\n",
      "0.9392296671867371 1\n",
      "0.9149558544158936 1\n",
      "62\n",
      "0.9706077575683594 1\n",
      "0.9706077575683594 1\n",
      "63\n",
      "0.9614734053611755 1\n",
      "0.8623353242874146 1\n",
      "64\n",
      "0.9695497751235962 1\n",
      "0.9695497751235962 1\n",
      "65\n",
      "0.9379175305366516 1\n",
      "0.8926998376846313 1\n",
      "66\n",
      "0.9980185031890869 1\n",
      "0.9980185031890869 1\n",
      "67\n",
      "0.9851964712142944 1\n",
      "0.9109933972358704 1\n",
      "68\n",
      "0.9635435342788696 1\n",
      "0.951465368270874 1\n",
      "69\n",
      "0.9798349738121033 1\n",
      "0.9798349738121033 1\n",
      "70\n",
      "0.9948580861091614 1\n",
      "0.9948580861091614 1\n",
      "71\n",
      "0.981759786605835 1\n",
      "0.9397312998771667 1\n",
      "72\n",
      "0.9996899366378784 1\n",
      "0.9996899366378784 1\n",
      "73\n",
      "0.9773643016815186 1\n",
      "0.9773643016815186 1\n",
      "74\n",
      "0.9884693026542664 1\n",
      "0.9884693026542664 1\n",
      "75\n",
      "0.9929918646812439 1\n",
      "0.9929918646812439 1\n",
      "76\n",
      "0.9699589014053345 1\n",
      "0.9699589014053345 1\n",
      "77\n",
      "0.9754946827888489 1\n",
      "0.9754946827888489 1\n",
      "78\n",
      "0.9774659872055054 1\n",
      "0.9774659872055054 1\n",
      "79\n",
      "0.9134806394577026 1\n",
      "0.0485563687980175 0\n",
      "80\n",
      "0.984316885471344 1\n",
      "0.7190691828727722 1\n",
      "81\n",
      "0.9631128907203674 1\n",
      "0.8703806400299072 1\n",
      "82\n",
      "0.9841644167900085 1\n",
      "0.9757381081581116 1\n",
      "83\n",
      "0.9883074760437012 1\n",
      "0.9883074760437012 1\n",
      "84\n",
      "0.9466296434402466 1\n",
      "0.9281226992607117 1\n",
      "85\n",
      "0.9644188284873962 1\n",
      "0.9098369479179382 1\n",
      "86\n",
      "0.9705948829650879 1\n",
      "0.8694899678230286 1\n",
      "87\n",
      "0.981160581111908 1\n",
      "0.981160581111908 1\n",
      "88\n",
      "0.939756453037262 1\n",
      "0.7725458741188049 1\n",
      "89\n",
      "0.9154510498046875 1\n",
      "0.9154510498046875 1\n",
      "90\n",
      "0.9776592254638672 1\n",
      "0.9541483521461487 1\n",
      "91\n",
      "0.9489279985427856 1\n",
      "0.9489279985427856 1\n",
      "92\n",
      "0.907098650932312 1\n",
      "0.8800190687179565 1\n",
      "93\n",
      "0.9360623359680176 1\n",
      "0.8773544430732727 1\n",
      "94\n",
      "0.9471924901008606 1\n",
      "0.9471924901008606 1\n",
      "95\n",
      "0.9851956963539124 1\n",
      "0.9851956963539124 1\n",
      "96\n",
      "0.9684873223304749 1\n",
      "0.9684873223304749 1\n",
      "97\n",
      "0.9166259169578552 1\n",
      "0.6641815304756165 1\n",
      "98\n",
      "0.9703193306922913 1\n",
      "0.9604556560516357 1\n",
      "99\n",
      "0.8980627059936523 1\n",
      "0.5490617156028748 1\n"
     ]
    }
   ],
   "source": [
    "wordstoptrank1001=[]\n",
    "wordstoptrank1002=[]\n",
    "for i in range(len(test1)):\n",
    "    wordstoptrank1001.append([])\n",
    "    for j in range(math.floor(len(test1[i])/text_group)):\n",
    "        wordstoptrank1001[-1].append(textrank.textrank(test1[i][j*text_group:(j+1)*text_group],100))\n",
    "for i in range(len(test2)):\n",
    "    wordstoptrank1002.append([])\n",
    "    for j in range(math.floor(len(test2[i])/text_group)):\n",
    "        wordstoptrank1002[-1].append(textrank.textrank(test2[i][j*text_group:(j+1)*text_group],100))\n",
    "        \n",
    "test1_att=[]\n",
    "test2_att=[]\n",
    "for i in range(len(wordstoptrank1001)):\n",
    "    test1_att.append([])\n",
    "    for j in range(len(wordstoptrank1001[i])):\n",
    "        x= [word_to_idx[k] for k in wordstoptrank1001[i][j]]\n",
    "        test1_att[-1].append(x)\n",
    "for i in range(len(wordstoptrank1002)):\n",
    "    test2_att.append([])\n",
    "    for j in range(len(wordstoptrank1002[i])):\n",
    "        x= [word_to_idx[k] for k in wordstoptrank1002[i][j]]\n",
    "        test2_att[-1].append(x)\n",
    "\n",
    "ra=[]\n",
    "ra1=[]\n",
    "for i in range(len(test_1)):\n",
    "    ra.append([])\n",
    "    ra1.append([])\n",
    "    for j in range(len(test_2)):\n",
    "        test_output=model(Variable(torch.LongTensor(test_1[i][0:text_group])).cuda(),Variable(torch.LongTensor(test_2[j][0:text_group])).cuda(),Variable(torch.LongTensor(test1_att[i][0])).cuda(),Variable(torch.LongTensor(test2_att[j][0])).cuda())\n",
    "\n",
    "        count1=0\n",
    "        count2=0\n",
    "        pred_y = torch.max(test_output.cpu(), 1)[1].data.numpy().squeeze()\n",
    "        if pred_y==1:\n",
    "            count2=1\n",
    "        else:\n",
    "            count2=0\n",
    "        for i2 in range(len(test_output.cpu())):\n",
    "            count1+=F.softmax(test_output.cpu()[i2])[1].data.numpy().squeeze()\n",
    "        ra[-1].append(count1)\n",
    "        ra1[-1].append(count2)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    print(i)\n",
    "    print(max(ra[-1]),max(ra1[-1]))\n",
    "    print(ra[-1][i],ra1[-1][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sysu502/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:34: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.9999608993530273 1\n",
      "0.9999608993530273 1\n",
      "1\n",
      "1.0 1\n",
      "0.9999949932098389 1\n",
      "2\n",
      "0.9999999403953552 1\n",
      "0.9059488773345947 1\n",
      "3\n",
      "0.9999914765357971 1\n",
      "0.9999695420265198 1\n",
      "4\n",
      "0.9999656081199646 1\n",
      "0.9999656081199646 1\n",
      "5\n",
      "1.0 1\n",
      "1.0 1\n",
      "6\n",
      "0.9999998211860657 1\n",
      "0.9999517202377319 1\n",
      "7\n",
      "1.0 1\n",
      "1.0 1\n",
      "8\n",
      "1.0 1\n",
      "1.0 1\n",
      "9\n",
      "0.999998152256012 1\n",
      "0.1848207414150238 0\n",
      "10\n",
      "0.9999995231628418 1\n",
      "0.9999995231628418 1\n",
      "11\n",
      "0.9999465346336365 1\n",
      "0.999932050704956 1\n",
      "12\n",
      "0.9999904632568359 1\n",
      "0.983428418636322 1\n",
      "13\n",
      "0.9999997019767761 1\n",
      "0.9609155654907227 1\n",
      "14\n",
      "0.9834640622138977 1\n",
      "0.9531801342964172 1\n",
      "15\n",
      "0.999940812587738 1\n",
      "0.3934744894504547 0\n",
      "16\n",
      "0.9999996423721313 1\n",
      "0.999896764755249 1\n",
      "17\n",
      "0.9999887347221375 1\n",
      "0.9933969974517822 1\n",
      "18\n",
      "0.9999999403953552 1\n",
      "0.12020143866539001 0\n",
      "19\n",
      "0.9999956488609314 1\n",
      "0.9992242455482483 1\n",
      "20\n",
      "0.9989805221557617 1\n",
      "0.38686880469322205 0\n",
      "21\n",
      "0.9995787143707275 1\n",
      "0.9995787143707275 1\n",
      "22\n",
      "1.0 1\n",
      "0.9110967516899109 1\n",
      "23\n",
      "0.9999997019767761 1\n",
      "0.9997721314430237 1\n",
      "24\n",
      "1.0 1\n",
      "0.9973542094230652 1\n",
      "25\n",
      "0.9999979734420776 1\n",
      "0.9999979734420776 1\n",
      "26\n",
      "0.9999984502792358 1\n",
      "0.9992405772209167 1\n",
      "27\n",
      "0.9999878406524658 1\n",
      "0.007779782172292471 0\n",
      "28\n",
      "0.9998000264167786 1\n",
      "0.9992136359214783 1\n",
      "29\n",
      "0.998607337474823 1\n",
      "0.9884458780288696 1\n",
      "30\n",
      "0.9999907612800598 1\n",
      "0.9999907612800598 1\n",
      "31\n",
      "0.9999743103981018 1\n",
      "0.9999743103981018 1\n",
      "32\n",
      "0.9995757341384888 1\n",
      "0.9220657348632812 1\n",
      "33\n",
      "0.9999842643737793 1\n",
      "0.7787246108055115 1\n",
      "34\n",
      "1.0 1\n",
      "0.9999996423721313 1\n",
      "35\n",
      "0.9999704360961914 1\n",
      "0.9848245978355408 1\n",
      "36\n",
      "0.9999996423721313 1\n",
      "0.9789822101593018 1\n",
      "37\n",
      "1.0 1\n",
      "0.9999930262565613 1\n",
      "38\n",
      "0.9999991655349731 1\n",
      "0.9999991655349731 1\n",
      "39\n",
      "1.0 1\n",
      "0.9954212307929993 1\n",
      "40\n",
      "0.9999980926513672 1\n",
      "0.9706243872642517 1\n",
      "41\n",
      "0.9999999403953552 1\n",
      "0.9999999403953552 1\n",
      "42\n",
      "0.9999943375587463 1\n",
      "0.9445796608924866 1\n",
      "43\n",
      "0.9999861121177673 1\n",
      "0.9999061226844788 1\n",
      "44\n",
      "1.0 1\n",
      "0.9998607039451599 1\n",
      "45\n",
      "0.9999116063117981 1\n",
      "0.9999116063117981 1\n",
      "46\n",
      "1.0 1\n",
      "1.0 1\n",
      "47\n",
      "0.9984946846961975 1\n",
      "0.9842153787612915 1\n",
      "48\n",
      "0.9999984502792358 1\n",
      "0.0010729770874604583 0\n",
      "49\n",
      "1.0 1\n",
      "0.9999533891677856 1\n",
      "50\n",
      "0.9959288239479065 1\n",
      "9.729922112455824e-07 0\n",
      "51\n",
      "1.0 1\n",
      "0.999921977519989 1\n",
      "52\n",
      "1.0 1\n",
      "0.984827995300293 1\n",
      "53\n",
      "0.9999998807907104 1\n",
      "0.999748170375824 1\n",
      "54\n",
      "0.9996710419654846 1\n",
      "0.9930521845817566 1\n",
      "55\n",
      "0.9999979138374329 1\n",
      "1.0099123528561904e-06 0\n",
      "56\n",
      "1.0 1\n",
      "0.9997881650924683 1\n",
      "57\n",
      "0.9998704195022583 1\n",
      "0.9996035695075989 1\n",
      "58\n",
      "1.0 1\n",
      "1.0 1\n",
      "59\n",
      "0.9999918937683105 1\n",
      "0.9991820454597473 1\n",
      "60\n",
      "0.9999969601631165 1\n",
      "0.9997634887695312 1\n",
      "61\n",
      "0.9999815821647644 1\n",
      "0.9993862509727478 1\n",
      "62\n",
      "1.0 1\n",
      "1.0 1\n",
      "63\n",
      "0.9998335838317871 1\n",
      "0.9709181785583496 1\n",
      "64\n",
      "0.9999998211860657 1\n",
      "0.9993156790733337 1\n",
      "65\n",
      "1.0 1\n",
      "1.0 1\n",
      "66\n",
      "0.9993966817855835 1\n",
      "0.006369274575263262 0\n",
      "67\n",
      "0.9999969601631165 1\n",
      "0.9753459095954895 1\n",
      "68\n",
      "0.9999992251396179 1\n",
      "0.9999905228614807 1\n",
      "69\n",
      "0.9999923706054688 1\n",
      "0.9999923706054688 1\n",
      "70\n",
      "1.0 1\n",
      "0.0031286205630749464 0\n",
      "71\n",
      "0.9999952912330627 1\n",
      "0.9998490810394287 1\n",
      "72\n",
      "1.0 1\n",
      "1.0 1\n",
      "73\n",
      "1.0 1\n",
      "1.0 1\n",
      "74\n",
      "0.9999995827674866 1\n",
      "0.9999995827674866 1\n",
      "75\n",
      "1.0 1\n",
      "1.0 1\n",
      "76\n",
      "0.9999998211860657 1\n",
      "0.9999958872795105 1\n",
      "77\n",
      "1.0 1\n",
      "1.0 1\n",
      "78\n",
      "0.9999995827674866 1\n",
      "0.9999995827674866 1\n",
      "79\n",
      "0.9999889731407166 1\n",
      "0.9999889731407166 1\n",
      "80\n",
      "0.9999999403953552 1\n",
      "0.9999999403953552 1\n",
      "81\n",
      "0.9999992847442627 1\n",
      "0.9980567693710327 1\n",
      "82\n",
      "0.999997615814209 1\n",
      "0.9943654537200928 1\n",
      "83\n",
      "0.9999860525131226 1\n",
      "3.2987551094265655e-05 0\n",
      "84\n",
      "0.9999972581863403 1\n",
      "0.9869570732116699 1\n",
      "85\n",
      "0.9995969533920288 1\n",
      "0.9958858489990234 1\n",
      "86\n",
      "1.0 1\n",
      "0.999995231628418 1\n",
      "87\n",
      "0.9998683929443359 1\n",
      "0.5366878509521484 1\n",
      "88\n",
      "0.9998706579208374 1\n",
      "0.9171967506408691 1\n",
      "89\n",
      "1.0 1\n",
      "0.0123853525146842 0\n",
      "90\n",
      "1.0 1\n",
      "1.0 1\n",
      "91\n",
      "0.9938511252403259 1\n",
      "0.9872546792030334 1\n",
      "92\n",
      "1.0 1\n",
      "0.9999322295188904 1\n",
      "93\n",
      "0.9976758360862732 1\n",
      "0.34130507707595825 0\n",
      "94\n",
      "0.9999807476997375 1\n",
      "0.9999807476997375 1\n",
      "95\n",
      "1.0 1\n",
      "0.9904071688652039 1\n",
      "96\n",
      "0.9999892115592957 1\n",
      "0.9999892115592957 1\n",
      "97\n",
      "0.9999973177909851 1\n",
      "0.9371073246002197 1\n",
      "98\n",
      "0.9999404549598694 1\n",
      "0.9996347427368164 1\n",
      "99\n",
      "0.9995748400688171 1\n",
      "0.9952521324157715 1\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "wordstoptrank1001=[]\n",
    "wordstoptrank1002=[]\n",
    "for i in range(len(test1)):\n",
    "    wordstoptrank1001.append(textrank.textrank(test1[i],100))\n",
    "for i in range(len(test2)):\n",
    "    wordstoptrank1002.append(textrank.textrank(test2[i],100))\n",
    "test1_att=[]\n",
    "test2_att=[]\n",
    "\n",
    "for i in range(len(wordstoptrank1001)):\n",
    "    x= [word_to_idx[k] for k in wordstoptrank1001[i]]\n",
    "    test1_att.append(x)\n",
    "for i in range(len(wordstoptrank1002)):\n",
    "    x= [word_to_idx[k] for k in wordstoptrank1002[i]]\n",
    "    test2_att.append(x)\n",
    "    \n",
    "ra=[]\n",
    "ra1=[]\n",
    "for i in range(len(test_1)):\n",
    "    ra.append([])\n",
    "    ra1.append([])\n",
    "    for j in range(len(test_2)):\n",
    "        test_output=model(Variable(torch.LongTensor(test_1[i])).cuda(),Variable(torch.LongTensor(test_2[j])).cuda(),Variable(torch.LongTensor(test1_att[i])).cuda(),Variable(torch.LongTensor(test2_att[j])).cuda())\n",
    "\n",
    "        count1=0\n",
    "        count2=0\n",
    "        pred_y = torch.max(test_output.cpu(), 1)[1].data.numpy().squeeze()\n",
    "        if pred_y==1:\n",
    "            count2=1\n",
    "        else:\n",
    "            count2=0\n",
    "        for i2 in range(len(test_output.cpu())):\n",
    "            count1+=F.softmax(test_output.cpu()[i2])[1].data.numpy().squeeze()\n",
    "        ra[-1].append(count1)\n",
    "        ra1[-1].append(count2)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    print(i)\n",
    "    print(max(ra[-1]),max(ra1[-1]))\n",
    "    print(ra[-1][i],ra1[-1][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall: 0.41\n",
      "precise 0.9534883720930233\n"
     ]
    }
   ],
   "source": [
    "ra1=np.matrix(ra[0:100][0:100])\n",
    "rat=ra1.T\n",
    "ac=0\n",
    "maac=0\n",
    "for i in range(len(ra1)):\n",
    "    r1=np.argmax(ra1[i])\n",
    "    r2=np.argmax(rat[i])\n",
    "    if r1==r2:\n",
    "        maac+=1\n",
    "    if r1==r2 and r1==i:\n",
    "        ac+=1\n",
    "recall=ac/len(ra1)\n",
    "precise=ac/maac\n",
    "print(\"recall:\",recall)\n",
    "print(\"precise\",precise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precise: 0.06795366795366796\n",
      "recall 0.88\n"
     ]
    }
   ],
   "source": [
    "all1=0\n",
    "correct1=0\n",
    "for i in range(len(ra1)):\n",
    "    for j in range(len(ra1[i])):\n",
    "        if ra1[i][j]==1:\n",
    "            all1+=1\n",
    "            if i==j:\n",
    "                correct1+=1\n",
    "print(\"precise:\",correct1/all1)\n",
    "print(\"recall\",correct1/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "input1 = open('/home/sysu502/Public/duxin/content_new961.txt', 'r',encoding=\"utf-8\")\n",
    "content=eval(input1.read())\n",
    "input1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sysu502/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "y=[]\n",
    "for i in range(len(content)):\n",
    "    y.append(0)\n",
    "x_train, x_test, y_train, y_test = train_test_split(content, y, train_size=861/961)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from __future__ import absolute_import, unicode_literals\n",
    "import sys\n",
    "from operator import itemgetter\n",
    "from collections import defaultdict\n",
    "\n",
    "class UndirectWeightedGraph:\n",
    "    d = 0.85\n",
    "\n",
    "    def __init__(self):\n",
    "        self.graph = defaultdict(list)\n",
    "\n",
    "    def addEdge(self, start, end, weight):\n",
    "        # use a tuple (start, end, weight) instead of a Edge object\n",
    "        self.graph[start].append((start, end, weight))\n",
    "        self.graph[end].append((end, start, weight))\n",
    "\n",
    "    def rank(self):\n",
    "        ws = defaultdict(float)\n",
    "        outSum = defaultdict(float)\n",
    "\n",
    "        wsdef = 1.0 / (len(self.graph) or 1.0)\n",
    "        for n, out in self.graph.items():\n",
    "            ws[n] = wsdef\n",
    "            outSum[n] = sum((e[2] for e in out), 0.0)\n",
    "\n",
    "        # this line for build stable iteration\n",
    "        sorted_keys = sorted(self.graph.keys())\n",
    "        for x in range(10):  # 10 iters\n",
    "            for n in sorted_keys:\n",
    "                s = 0\n",
    "                for e in self.graph[n]:\n",
    "                    s += e[2] / outSum[e[1]] * ws[e[1]]\n",
    "                ws[n] = (1 - self.d) + self.d * s\n",
    "\n",
    "        (min_rank, max_rank) = (sys.float_info[0], sys.float_info[3])\n",
    "\n",
    "        for w in ws.values():\n",
    "            if w < min_rank:\n",
    "                min_rank = w\n",
    "            if w > max_rank:\n",
    "                max_rank = w\n",
    "\n",
    "        for n, w in ws.items():\n",
    "            # to unify the weights, don't *100.\n",
    "            ws[n] = (w - min_rank / 10.0) / (max_rank - min_rank / 10.0)\n",
    "\n",
    "        return ws\n",
    "\n",
    "class TextRank:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.span = 3\n",
    "\n",
    "    def pairfilter(self, wp):\n",
    "        return (wp.flag in self.pos_filt and len(wp.word.strip()) >= 2\n",
    "                and wp.word.lower() not in self.stop_words)\n",
    "\n",
    "    def textrank(self, sentences, topK=20):\n",
    "        \"\"\"\n",
    "        Extract keywords from sentence using TextRank algorithm.\n",
    "        Parameter:\n",
    "            - topK: return how many top keywords. `None` for all possible words.\n",
    "            - withWeight: if True, return a list of (word, weight);\n",
    "                          if False, return a list of words.\n",
    "            - allowPOS: the allowed POS list eg. ['ns', 'n', 'vn', 'v'].\n",
    "                        if the POS of w is not in this list, it will be filtered.\n",
    "            - withFlag: if True, return a list of pair(word, weight) like posseg.cut\n",
    "                        if False, return a list of words\n",
    "        \"\"\"\n",
    "#         self.pos_filt = frozenset(allowPOS)\n",
    "        g = UndirectWeightedGraph()\n",
    "        cm = defaultdict(int)\n",
    "#         words = tuple(self.tokenizer.cut(sentence))\n",
    "        for i in range(len(sentences)):\n",
    "            words=sentences[i].split(\" \")\n",
    "            for j, wp in enumerate(words):\n",
    "                for k in range(j, j + self.span):\n",
    "                    if k >= len(words):\n",
    "                        break\n",
    "                    cm[(wp, words[k])] += 1\n",
    "\n",
    "        for terms, w in cm.items():\n",
    "            g.addEdge(terms[0], terms[1], w)\n",
    "        nodes_rank = g.rank()\n",
    "        tags = sorted(nodes_rank, key=nodes_rank.__getitem__, reverse=True)\n",
    "\n",
    "        if topK:\n",
    "            return tags[:topK]\n",
    "        else:\n",
    "            return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import gensim\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "train1=[]\n",
    "train2=[]\n",
    "train_1=[]\n",
    "train_2=[]\n",
    "trainn_1=[]\n",
    "trainn_2=[]\n",
    "c_set=30\n",
    "min_len=6\n",
    "max_len=100\n",
    "min_rate=0.6\n",
    "max_rate=0.8\n",
    "base_count=(max_len-min_len)/(max_rate-min_rate)\n",
    "mark_count=10\n",
    "comlen=10\n",
    "wordstop100=[]\n",
    "train_tag=[]\n",
    "trainn_tag=[]\n",
    "\n",
    "for i in range(len(x_train)):\n",
    "    random.shuffle(x_train[i])\n",
    "    \n",
    "for i in range(len(x_train)):\n",
    "    print(i)\n",
    "    x=\"\"\n",
    "    for k in range(comlen):\n",
    "        if k==0:\n",
    "            x+=x_train[i][k]\n",
    "        else:\n",
    "            x+=\" \"+x_train[i][k]\n",
    "        if len(''.join(x.split(' ')))>200:\n",
    "            break\n",
    "    j=comlen\n",
    "    while j<180:\n",
    "        y=\"\"\n",
    "        for k in range(comlen):\n",
    "            if k==0:\n",
    "                y+=x_train[i][j+k]\n",
    "            else:\n",
    "                y+=\" \"+x_train[i][j+k]\n",
    "            if len(''.join(y.split(' ')))>200:\n",
    "                break\n",
    "        train_1.append(x)\n",
    "        train_2.append(y)\n",
    "        train_tag.append(i)\n",
    "        j+=1+k\n",
    "    j=i+1\n",
    "    while j<len(x_train) and j<i+40:\n",
    "        y=\"\"\n",
    "        if len(x_train[j])<10:\n",
    "            j+=1\n",
    "            continue\n",
    "        l=random.sample(range(len(x_train[j])),comlen)\n",
    "        for k in range(len(l)):\n",
    "            if k==0:\n",
    "                y+=x_train[j][l[k]]\n",
    "            else:\n",
    "                y+=\" \"+x_train[j][l[k]]\n",
    "            if len(''.join(y.split(' ')))>200:\n",
    "                break\n",
    "        trainn_1.append(x)\n",
    "        trainn_2.append(y)\n",
    "        trainn_tag.append((i,j))\n",
    "        j+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import gensim\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "train1=[]\n",
    "train2=[]\n",
    "\n",
    "train_1=[]\n",
    "train_2=[]\n",
    "\n",
    "trainn_1=[]\n",
    "trainn_2=[]\n",
    "textrank=TextRank()\n",
    "text_group=5\n",
    "comlen=10\n",
    "wordstoprank1001=[]\n",
    "wordstoprank1002=[]\n",
    "train_tag=[]\n",
    "trainn_tag=[]\n",
    "\n",
    "for i in range(len(x_train)):\n",
    "    random.shuffle(x_train[i])\n",
    "    \n",
    "for i in range(len(x_train)):\n",
    "    j=0\n",
    "    train1.append([])\n",
    "    while j<len(x_train[i]):\n",
    "        y=\"\"\n",
    "        for k in range(comlen):\n",
    "            if j+k<len(x_train[i]):\n",
    "                if k==0:\n",
    "                    y+=x_train[i][j+k]\n",
    "                else:\n",
    "                    y+=\" \"+x_train[i][j+k]\n",
    "                if len(''.join(y.split(' ')))>200:\n",
    "                    break\n",
    "            else:\n",
    "                break\n",
    "        train1[-1].append(y)\n",
    "        j+=1+k\n",
    "\n",
    "i=0\n",
    "while i<len(train1):\n",
    "#     if len(train1[i])<20:\n",
    "#         del train1[i]\n",
    "#         continue\n",
    "    train2.append([])\n",
    "    x=math.floor(len(train1[i])/2)\n",
    "    train2[-1]=train1[i][x:]\n",
    "    train1[i]=train1[i][0:x]\n",
    "    i+=1\n",
    "for i in range(len(train1)):\n",
    "    wordstoprank1001.append([])\n",
    "    for j in range(math.floor(len(train1[i])/text_group)):\n",
    "        wordstoprank1001[-1].append(textrank.textrank(train1[i][j*text_group:(j+1)*text_group],100))\n",
    "for i in range(len(train2)):\n",
    "    wordstoprank1002.append([])\n",
    "    for j in range(math.floor(len(train2[i])/text_group)):\n",
    "        wordstoprank1002[-1].append(textrank.textrank(train2[i][j*text_group:(j+1)*text_group],100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import gensim\n",
    "import random\n",
    "from collections import Counter\n",
    "# from langconv import *\n",
    "# def tradition2simple(line):\n",
    "#     # 将繁体转换成简体\n",
    "#     line = Converter('zh-hans').convert(line)\n",
    "#     return line\n",
    "\n",
    "# EMBEDDING_FILE='/home/sysu502/Public/duxin/wiki/wiki/word2vec_wiki.model'\n",
    "# mod = gensim.models.deprecated.word2vec.Word2Vec.load(EMBEDDING_FILE)\n",
    "test_1=[]\n",
    "test_2=[]\n",
    "testn_1=[]\n",
    "testn_2=[]\n",
    "c_set=30\n",
    "min_len=6\n",
    "max_len=100\n",
    "min_rate=0.6\n",
    "max_rate=0.8\n",
    "base_count=(max_len-min_len)/(max_rate-min_rate)\n",
    "mark_count=10\n",
    "comlen=10\n",
    "wordstopt100=[]\n",
    "test_tag=[]\n",
    "testn_tag=[]\n",
    "\n",
    "for i in range(len(x_test)):\n",
    "    random.shuffle(x_test[i])\n",
    "    \n",
    "#记得处理停用词和标点\n",
    "for i in range(len(x_test)):\n",
    "    print(i)\n",
    "    x=\"\"\n",
    "    for k in range(comlen):\n",
    "        if k==0:\n",
    "            x+=x_test[i][k]\n",
    "        else:\n",
    "            x+=\" \"+x_test[i][k]\n",
    "        if len(''.join(x.split(' ')))>200:\n",
    "            break\n",
    "    j=comlen\n",
    "    while j<180:\n",
    "        y=\"\"\n",
    "        for k in range(comlen):\n",
    "            if k==0:\n",
    "                y+=x_test[i][j+k]\n",
    "            else:\n",
    "                y+=\" \"+x_test[i][j+k]\n",
    "            if len(''.join(y.split(' ')))>200:\n",
    "                break\n",
    "        test_1.append(x)\n",
    "        test_2.append(y)\n",
    "        test_tag.append(i)\n",
    "        j+=1+k\n",
    "    j=i+1\n",
    "    while j<len(x_test):\n",
    "        y=\"\"\n",
    "        if len(x_test[j])<10:\n",
    "            j+=1\n",
    "            continue\n",
    "        l=random.sample(range(len(x_test[j])),comlen)\n",
    "        for k in range(len(l)):\n",
    "            if k==0:\n",
    "                y+=x_test[j][l[k]]\n",
    "            else:\n",
    "                y+=\" \"+x_test[j][l[k]]\n",
    "            if len(''.join(y.split(' ')))>200:\n",
    "                break\n",
    "        testn_1.append(x)\n",
    "        testn_2.append(y)\n",
    "        testn_tag.append((i,j))\n",
    "        j+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import gensim\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "test1=[]\n",
    "test2=[]\n",
    "\n",
    "test_1=[]\n",
    "test_2=[]\n",
    "\n",
    "testn_1=[]\n",
    "testn_2=[]\n",
    "\n",
    "comlen=10\n",
    "wordstoptrank1001=[]\n",
    "wordstoptrank1002=[]\n",
    "test_tag=[]\n",
    "testn_tag=[]\n",
    "\n",
    "for i in range(len(x_test)):\n",
    "    random.shuffle(x_test[i])\n",
    "    \n",
    "for i in range(len(x_test)):\n",
    "    j=0\n",
    "    test1.append([])\n",
    "    while j<len(x_test[i]):\n",
    "        y=\"\"\n",
    "        for k in range(comlen):\n",
    "            if j+k<len(x_test[i]):\n",
    "                if k==0:\n",
    "                    y+=x_test[i][j+k]\n",
    "                else:\n",
    "                    y+=\" \"+x_test[i][j+k]\n",
    "                if len(''.join(y.split(' ')))>200:\n",
    "                    break\n",
    "            else:\n",
    "                break\n",
    "        test1[-1].append(y)\n",
    "        j+=1+k\n",
    "\n",
    "i=0\n",
    "while i<len(test1):\n",
    "#     if len(train1[i])<20:\n",
    "#         del train1[i]\n",
    "#         continue\n",
    "    test2.append([])\n",
    "    x=math.floor(len(test1[i])/2)\n",
    "    test2[-1]=test1[i][x:]\n",
    "    test1[i]=test1[i][0:x]\n",
    "    i+=1\n",
    "for i in range(len(test1)):\n",
    "    wordstoptrank1001.append([])\n",
    "    for j in range(math.floor(len(test1[i])/text_group)):\n",
    "        wordstoptrank1001[-1].append(textrank.textrank(test1[i][j*text_group:(j+1)*text_group],100))\n",
    "for i in range(len(test2)):\n",
    "    wordstoptrank1002.append([])\n",
    "    for j in range(math.floor(len(test2[i])/text_group)):\n",
    "        wordstoptrank1002[-1].append(textrank.textrank(test2[i][j*text_group:(j+1)*text_group],100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import gensim\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "test1=[]\n",
    "test2=[]\n",
    "\n",
    "input1 = open('/home/sysu2018/Public/duxin/content_new961_idf.txt', 'r',encoding=\"utf-8\")\n",
    "idf=eval(input1.read())\n",
    "input1.close()\n",
    "\n",
    "# test_1=[]\n",
    "# test_2=[]\n",
    "\n",
    "# testn_1=[]\n",
    "# testn_2=[]\n",
    "\n",
    "textrank=TextRank()\n",
    "\n",
    "comlen=10\n",
    "wordstopt1001=[]\n",
    "wordstopt1002=[]\n",
    "wordstopttf1001=[]\n",
    "wordstopttf1002=[]\n",
    "wordstoptrank1001=[]\n",
    "wordstoptrank1002=[]\n",
    "# test_tag=[]\n",
    "# testn_tag=[]\n",
    "\n",
    "for i in range(len(x_test)):\n",
    "    random.shuffle(x_test[i])\n",
    "\n",
    "for i in range(len(x_test)):\n",
    "    j=0\n",
    "    test1.append([])\n",
    "    while j<len(x_test[i]):    \n",
    "        y=\"\"\n",
    "        for k in range(comlen):\n",
    "            if j+k<len(x_test[i]):\n",
    "                if k==0:\n",
    "                    y+=x_test[i][j+k]\n",
    "                else:\n",
    "                    y+=\" \"+x_test[i][j+k]\n",
    "                if len(''.join(y.split(' ')))>200:\n",
    "                    break\n",
    "            else:\n",
    "                break\n",
    "        test1[-1].append(y)\n",
    "        j+=1+k\n",
    "\n",
    "i=0\n",
    "while i<len(test1):\n",
    "#     if len(test1[i])<20:\n",
    "#         del test1[i]\n",
    "#         continue\n",
    "    test2.append([])\n",
    "    x=math.floor(len(test1[i])/2)\n",
    "    test2[-1]=test1[i][x:]\n",
    "    test1[i]=test1[i][0:x]\n",
    "    i+=1\n",
    "for i in range(len(test1)):\n",
    "    wordstopt1001.append(Counter())\n",
    "    for j in range(len(test1[i])):\n",
    "        words=test1[i][j].split(\" \")\n",
    "        for k in range(len(words)):\n",
    "            wordstopt1001[-1][words[k]]+=1\n",
    "    wordstopt1001[-1]=Counter(dict(wordstopt1001[-1].most_common(800)))\n",
    "for i in range(len(test2)):\n",
    "    wordstopt1002.append(Counter())\n",
    "    for j in range(len(test2[i])):\n",
    "        words=test2[i][j].split(\" \")\n",
    "        for k in range(len(words)):\n",
    "            wordstopt1002[-1][words[k]]+=1\n",
    "    wordstopt1002[-1]=Counter(dict(wordstopt1002[-1].most_common(800)))\n",
    "\n",
    "for i in range(len(test1)):\n",
    "    wordstopttf1001.append(Counter())\n",
    "    for j in range(len(test1[i])):\n",
    "        words=test1[i][j].split(\" \")\n",
    "        for k in range(len(words)):\n",
    "            wordstopttf1001[-1][words[k]]+=1\n",
    "    for key in wordstopttf1001[-1]:\n",
    "        wordstopttf1001[-1][key]=wordstopttf1001[-1][key]*idf[key]\n",
    "    wordstopttf1001[-1]=Counter(dict(wordstopttf1001[-1].most_common(800)))\n",
    "for i in range(len(test2)):\n",
    "    wordstopttf1002.append(Counter())\n",
    "    for j in range(len(test2[i])):\n",
    "        words=test2[i][j].split(\" \")\n",
    "        for k in range(len(words)):\n",
    "            wordstopttf1002[-1][words[k]]+=1\n",
    "    for key in wordstopttf1002[-1]:\n",
    "        wordstopttf1002[-1][key]=wordstopttf1002[-1][key]*idf[key]\n",
    "    wordstopttf1002[-1]=Counter(dict(wordstopttf1002[-1].most_common(800)))\n",
    "\n",
    "for i in range(len(test1)):\n",
    "    wordstoptrank1001.append(textrank.textrank(test1[i],800))\n",
    "for i in range(len(test2)):\n",
    "    wordstoptrank1002.append(textrank.textrank(test2[i],800))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(len(train_1))\n",
    "print(len(trainn_1))\n",
    "print(len(test_1))\n",
    "print(len(testn_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(wordstopttf1002)):\n",
    "    if len(wordstopttf1002[i])<800:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n"
     ]
    }
   ],
   "source": [
    "test1=[]\n",
    "test2=[]\n",
    "wordstop1001=[]\n",
    "wordstop1002=[]\n",
    "for i in range(len(x_test)):\n",
    "    print(i)\n",
    "    test1.append([])\n",
    "    j=0\n",
    "    while j<len(x_test[i]):\n",
    "        y=\"\"\n",
    "        for k in range(10):\n",
    "            if j+k<len(x_test[i]):\n",
    "                if k==0:\n",
    "                    y+=x_test[i][j+k]\n",
    "                else:\n",
    "                    y+=\" \"+x_test[i][j+k]\n",
    "                if len(''.join(y.split(' ')))>200:\n",
    "                    break\n",
    "            else:\n",
    "                break\n",
    "            \n",
    "        test1[-1].append(y)\n",
    "        j+=1+k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "i=0\n",
    "while i<len(test1):\n",
    "    if len(test1[i])<20:\n",
    "        del test1[i]\n",
    "        continue\n",
    "    test2.append([])\n",
    "    x=math.floor(len(test1[i])/2)\n",
    "    test2[-1]=test1[i][x:]\n",
    "    test1[i]=test1[i][0:x]\n",
    "    i+=1\n",
    "for i in range(len(test1)):\n",
    "    wordstop1001.append(Counter())\n",
    "    for j in range(len(test1[i])):\n",
    "        words=test1[i][j].split(\" \")\n",
    "        for k in range(len(words)):\n",
    "            wordstop1001[-1][words[k]]+=1\n",
    "    wordstop1001[-1]=Counter(dict(wordstop1001[-1].most_common(100)))\n",
    "for i in range(len(test2)):\n",
    "    wordstop1002.append(Counter())\n",
    "    for j in range(len(test2[i])):\n",
    "        words=test2[i][j].split(\" \")\n",
    "        for k in range(len(words)):\n",
    "            wordstop1002[-1][words[k]]+=1\n",
    "    wordstop1002[-1]=Counter(dict(wordstop1002[-1].most_common(100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sysu2018/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:100: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/sysu2018/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:107: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/sysu2018/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:111: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/sysu2018/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:115: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.7979478872504694 0.9260204081632653 0.8374118804931641 0.8468661308288574 0.8765908479690552\n",
      "0.7979478872504694 0.9260204081632653 0.8263314962387085 0.8200780749320984 0.8439550995826721\n",
      "1\n",
      "0.680796604577478 0.8321678321678322 0.7534127831459045 0.6445483565330505 0.7802523970603943\n",
      "0.5447516265000112 0.6153846153846154 0.6578428149223328 0.5015740394592285 0.7349832057952881\n",
      "2\n",
      "0.8522536060362935 0.9204204204204204 0.8029087781906128 0.7707353234291077 0.8022612929344177\n",
      "0.8522536060362935 0.9204204204204204 0.7379116415977478 0.7707353234291077 0.7227002382278442\n",
      "3\n",
      "0.9105122293949127 0.9952 0.8031391501426697 0.8136804699897766 0.8482223153114319\n",
      "0.9105122293949127 0.9952 0.7830571532249451 0.7749911546707153 0.7628447413444519\n",
      "4\n",
      "0.8581468451860522 0.9669421487603306 0.8535425662994385 0.8759013414382935 0.8839542865753174\n",
      "0.8581468451860522 0.9669421487603306 0.8460573554039001 0.8419272899627686 0.8127007484436035\n",
      "5\n",
      "0.6405529680232016 0.7646153846153846 0.78849196434021 0.7784147262573242 0.7901313304901123\n",
      "0.6405529680232016 0.7646153846153846 0.7043179273605347 0.5931495428085327 0.749581515789032\n",
      "6\n",
      "0.7624785016024462 0.8923611111111112 0.7665157914161682 0.7289422750473022 0.785565972328186\n",
      "0.7624785016024462 0.8923611111111112 0.7654191255569458 0.7289422750473022 0.785565972328186\n",
      "7\n",
      "0.7813672145506493 0.849624060150376 0.821739673614502 0.8253248333930969 0.8256378173828125\n",
      "0.6638966964655801 0.7710526315789473 0.821739673614502 0.7220958471298218 0.7867897152900696\n",
      "8\n",
      "0.8239371386236277 0.9462585034013605 0.8638062477111816 0.8750339150428772 0.8791471123695374\n",
      "0.819245266109291 0.9358333333333333 0.8426281809806824 0.8476321697235107 0.849496603012085\n",
      "9\n",
      "0.6023306966471401 0.7090909090909091 0.765264630317688 0.7755163908004761 0.7758506536483765\n",
      "0.5601242663519657 0.6528925619834711 0.726077675819397 0.7295165061950684 0.7410500645637512\n",
      "10\n",
      "0.936006055213511 0.984375 0.9109584093093872 0.9250112771987915 0.960280179977417\n",
      "0.936006055213511 0.984375 0.9109584093093872 0.9250112771987915 0.960280179977417\n",
      "11\n",
      "0.935137914639826 0.9720710059171598 0.889211118221283 0.957368016242981 0.942230224609375\n",
      "0.935137914639826 0.9720710059171598 0.889211118221283 0.9147858023643494 0.942230224609375\n",
      "12\n",
      "0.897271961973044 0.9774436090225563 0.9136538505554199 0.9699951410293579 0.9362744688987732\n",
      "0.897271961973044 0.9774436090225563 0.9136538505554199 0.9699951410293579 0.9362744688987732\n",
      "13\n",
      "0.6269457026231604 0.7315689981096408 0.8209158778190613 0.8410516381263733 0.7876259088516235\n",
      "0.5940729296380665 0.6899810964083176 0.7582272887229919 0.806108295917511 0.7876259088516235\n",
      "14\n",
      "0.6156021212148418 0.7391304347826086 0.718817949295044 0.792779266834259 0.7505581974983215\n",
      "0.6156021212148418 0.7391304347826086 0.7185439467430115 0.792779266834259 0.7505581974983215\n",
      "15\n",
      "0.5447055796531084 0.6397058823529411 0.7599799036979675 0.7217909097671509 0.7848923206329346\n",
      "0.5447055796531084 0.6397058823529411 0.696414053440094 0.6826526522636414 0.6772803664207458\n",
      "16\n",
      "0.883824161378526 0.9684711481261155 0.7883862853050232 0.7437679171562195 0.8033210635185242\n",
      "0.883824161378526 0.9684711481261155 0.7813093066215515 0.6345503926277161 0.7814085483551025\n",
      "17\n",
      "0.6181720249167282 0.7222222222222222 0.7374830842018127 0.8165786266326904 0.7930120229721069\n",
      "0.6181720249167282 0.7222222222222222 0.7066986560821533 0.7104806900024414 0.6997635364532471\n",
      "18\n",
      "0.6542091463184492 0.7563025210084033 0.8045474290847778 0.7637559175491333 0.8505374193191528\n",
      "0.6262330500193996 0.7270408163265306 0.6956192851066589 0.7267086505889893 0.7339788675308228\n",
      "19\n",
      "0.6748549928554832 0.7747456059204441 0.7627635598182678 0.8048426508903503 0.773649275302887\n",
      "0.6748549928554832 0.7747456059204441 0.7587457299232483 0.7439718246459961 0.74606853723526\n",
      "20\n",
      "0.8208932185714896 0.961038961038961 0.7955813407897949 0.7182795405387878 0.8550798892974854\n",
      "0.7911002570686262 0.9256198347107438 0.7262201309204102 0.6013328433036804 0.7546712160110474\n",
      "21\n",
      "0.8967642622448353 0.9488388969521045 0.91651850938797 0.9132936000823975 0.9266852736473083\n",
      "0.8967642622448353 0.9488388969521045 0.91651850938797 0.8970972299575806 0.9266852736473083\n",
      "22\n",
      "0.6791331443388687 0.7925824175824175 0.8424515724182129 0.8465211987495422 0.8511946797370911\n",
      "0.6791331443388687 0.7873520710059172 0.8424515724182129 0.8465211987495422 0.8250554203987122\n",
      "23\n",
      "0.6908360600324083 0.8062130177514792 0.7619154453277588 0.7613648176193237 0.760007381439209\n",
      "0.6908360600324083 0.8062130177514792 0.7336345314979553 0.7475882768630981 0.7498406767845154\n",
      "24\n",
      "0.951823280173905 1.0 0.8855166435241699 0.9566739797592163 0.8581483960151672\n",
      "0.951823280173905 1.0 0.8855166435241699 0.9566739797592163 0.8581483960151672\n",
      "25\n",
      "0.8479785036972978 0.9318181818181818 0.8260394334793091 0.7846755981445312 0.8746508359909058\n",
      "0.7932599311285286 0.8863636363636364 0.8119909763336182 0.7691636681556702 0.8182439804077148\n",
      "26\n",
      "0.6171560581449581 0.7272727272727273 0.820019006729126 0.801263153553009 0.8613865971565247\n",
      "0.6171560581449581 0.7272727272727273 0.6955597400665283 0.7049537301063538 0.7972772121429443\n",
      "27\n",
      "0.6245078367650697 0.7235294117647059 0.8263716697692871 0.8069790005683899 0.8446404933929443\n",
      "0.6245078367650697 0.7235294117647059 0.7583532333374023 0.7855924963951111 0.8263757228851318\n",
      "28\n",
      "0.8019017509140691 0.9097633136094675 0.8005183935165405 0.8459475636482239 0.8790730237960815\n",
      "0.8019017509140691 0.9097633136094675 0.8005183935165405 0.8459475636482239 0.7964557409286499\n",
      "29\n",
      "0.7464177049696445 0.8741976893453145 0.8480319976806641 0.87493497133255 0.887252926826477\n",
      "0.7295777553202291 0.85 0.8480319976806641 0.87493497133255 0.8638260364532471\n",
      "30\n",
      "0.8678465185221285 0.9301470588235294 0.8821763396263123 0.9605340361595154 0.8989777565002441\n",
      "0.8678465185221285 0.9301470588235294 0.8502081036567688 0.9605340361595154 0.8989777565002441\n",
      "31\n",
      "0.5603743842453696 0.6125 0.7720564603805542 0.7541701793670654 0.7665255665779114\n",
      "0.5603743842453696 0.6125 0.6735363006591797 0.7541701793670654 0.7484702467918396\n",
      "32\n",
      "0.5456819962278513 0.6185383244206774 0.7792911529541016 0.7469666004180908 0.7865787148475647\n",
      "0.5456819962278513 0.6185383244206774 0.731127917766571 0.7054294347763062 0.7178071737289429\n",
      "33\n",
      "0.8391332675543426 0.9251700680272109 0.8449458479881287 0.8528003096580505 0.7954127788543701\n",
      "0.8391332675543426 0.9251700680272109 0.8449458479881287 0.8528003096580505 0.7954127788543701\n",
      "34\n",
      "0.6847916690670457 0.7941787941787942 0.8159631490707397 0.8072021007537842 0.8565865159034729\n",
      "0.6713353732758637 0.7815924032140248 0.7278821468353271 0.710700511932373 0.7593399286270142\n",
      "35\n",
      "0.9226772530612721 0.9946127946127946 0.8546403646469116 0.8412895798683167 0.872719407081604\n",
      "0.9226772530612721 0.9946127946127946 0.8546403646469116 0.8412895798683167 0.872719407081604\n",
      "36\n",
      "0.6656120736425852 0.753393665158371 0.8127256631851196 0.7576290965080261 0.8326785564422607\n",
      "0.612555138072839 0.6797385620915033 0.7817692756652832 0.5867623090744019 0.7876163721084595\n",
      "37\n",
      "0.7163019680839271 0.8038194444444444 0.8068726062774658 0.8115850687026978 0.8259699940681458\n",
      "0.7163019680839271 0.8038194444444444 0.7478438019752502 0.7672367095947266 0.8165655136108398\n",
      "38\n",
      "0.858310857809386 0.9382716049382716 0.8925172686576843 0.9491238594055176 0.9017175436019897\n",
      "0.858310857809386 0.9269005847953217 0.8863077163696289 0.947749674320221 0.9017175436019897\n",
      "39\n",
      "0.6394301034851728 0.7474048442906575 0.7885720729827881 0.7264779806137085 0.8077400326728821\n",
      "0.6394301034851728 0.7474048442906575 0.6755198240280151 0.698828935623169 0.7315090298652649\n",
      "40\n",
      "0.8136049438336933 0.9407407407407408 0.8433651328086853 0.8710485100746155 0.8564590811729431\n",
      "0.7615085667243396 0.8709856035437431 0.7957651615142822 0.7929099202156067 0.8416439294815063\n",
      "41\n",
      "0.7652558272238821 0.8359375 0.7545266151428223 0.7049638032913208 0.7831348776817322\n",
      "0.7652558272238821 0.8359375 0.7545266151428223 0.6541913747787476 0.7831348776817322\n",
      "42\n",
      "0.889294188016101 0.9648803329864725 0.8737718462944031 0.9248558282852173 0.9145782589912415\n",
      "0.889294188016101 0.9648803329864725 0.8547601103782654 0.9248558282852173 0.8711450099945068\n",
      "43\n",
      "0.5193964403734862 0.5454545454545454 0.742755651473999 0.7682543992996216 0.7146667242050171\n",
      "0.4936996129655392 0.5075757575757576 0.68897545337677 0.625818133354187 0.7146667242050171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44\n",
      "0.5933344975162885 0.6929824561403509 0.6817144751548767 0.7157018184661865 0.705075204372406\n",
      "0.50983903862727 0.5705128205128205 0.5069482326507568 0.5671470165252686 0.5417092442512512\n",
      "45\n",
      "0.6247348983467854 0.6974789915966386 0.740994930267334 0.7663410305976868 0.690714418888092\n",
      "0.5861049646213408 0.6568627450980392 0.7150290012359619 0.6987098455429077 0.6131554841995239\n",
      "46\n",
      "0.8324261544246434 0.9506944444444444 0.9167168140411377 0.9129583239555359 0.9155132174491882\n",
      "0.81524210236058 0.9408536585365853 0.8598092198371887 0.8912459015846252 0.8957359194755554\n",
      "47\n",
      "0.740212342677772 0.8742138364779874 0.8197256922721863 0.8660736083984375 0.8831551671028137\n",
      "0.740212342677772 0.8742138364779874 0.8197256922721863 0.8660736083984375 0.8831551671028137\n",
      "48\n",
      "0.5255316419309749 0.567193675889328 0.7492517828941345 0.7180669903755188 0.742656409740448\n",
      "0.4945468542194276 0.5606060606060606 0.6700057983398438 0.6205667853355408 0.6613075137138367\n",
      "49\n",
      "0.90681973320211 0.9574759945130316 0.8936912417411804 0.9294580221176147 0.937534511089325\n",
      "0.90681973320211 0.9574759945130316 0.8936912417411804 0.9294580221176147 0.937534511089325\n",
      "50\n",
      "0.8611821355039461 0.9537037037037037 0.8778741359710693 0.8817996978759766 0.8836119174957275\n",
      "0.8611821355039461 0.9537037037037037 0.8534718751907349 0.8582723736763 0.8768942356109619\n",
      "51\n",
      "0.8310237735663856 0.9170918367346939 0.8513324856758118 0.9491987824440002 0.8231328725814819\n",
      "0.8310237735663856 0.9170918367346939 0.8513324856758118 0.9491987824440002 0.8231328725814819\n",
      "52\n",
      "0.6986527598286554 0.817475320310859 0.7819272875785828 0.7832554578781128 0.8326373100280762\n",
      "0.6986527598286554 0.817475320310859 0.7506929636001587 0.7679368853569031 0.7746902704238892\n",
      "53\n",
      "0.853941069073197 0.9250780437044746 0.8599099516868591 0.92392498254776 0.8457086682319641\n",
      "0.853941069073197 0.9250780437044746 0.8599099516868591 0.92392498254776 0.8364042043685913\n",
      "54\n",
      "0.6925586493855173 0.7727272727272727 0.7910071015357971 0.794379472732544 0.8356219530105591\n",
      "0.6210745509620756 0.67 0.74502032995224 0.7345202565193176 0.7352163195610046\n",
      "55\n",
      "0.6970628466699603 0.7928994082840237 0.8099083304405212 0.7534781694412231 0.8180943727493286\n",
      "0.6970628466699603 0.7928994082840237 0.8099083304405212 0.7514991760253906 0.7986212968826294\n",
      "56\n",
      "0.8082721179181879 0.9363636363636364 0.8603283762931824 0.8626155257225037 0.8631884455680847\n",
      "0.8082721179181879 0.9363636363636364 0.8102311491966248 0.8082298040390015 0.8101854920387268\n",
      "57\n",
      "0.9408462211765249 0.9758454106280193 0.9105238318443298 0.962684690952301 0.9470825791358948\n",
      "0.9408462211765249 0.9758454106280193 0.9105238318443298 0.962684690952301 0.9470825791358948\n",
      "58\n",
      "0.8621436037554019 0.9772727272727273 0.8637640476226807 0.880948007106781 0.8996439576148987\n",
      "0.8621436037554019 0.9727891156462585 0.8602814674377441 0.880948007106781 0.8996439576148987\n",
      "59\n",
      "0.7604097742853421 0.8333333333333334 0.7541387677192688 0.8495713472366333 0.8167381286621094\n",
      "0.7604097742853421 0.8333333333333334 0.7541387677192688 0.7208306789398193 0.8167381286621094\n",
      "60\n",
      "0.9725374070241263 0.9815789473684211 0.958885908126831 0.9808710217475891 0.9286001920700073\n",
      "0.9725374070241263 0.9815789473684211 0.958885908126831 0.9808710217475891 0.9286001920700073\n",
      "61\n",
      "0.9630492102504169 0.9940828402366864 0.9104537963867188 0.9031326770782471 0.8937870264053345\n",
      "0.9630492102504169 0.9940828402366864 0.9104537963867188 0.887483537197113 0.8937870264053345\n",
      "62\n",
      "0.616418349404448 0.6895424836601307 0.7447711825370789 0.8005635738372803 0.7412448525428772\n",
      "0.5820252296367758 0.6637426900584795 0.7376685738563538 0.653677761554718 0.6798343062400818\n",
      "63\n",
      "0.6041313972417265 0.6133333333333333 0.8184537887573242 0.7829490303993225 0.8276934027671814\n",
      "0.6041313972417265 0.6133333333333333 0.7153658270835876 0.6198754906654358 0.7405469417572021\n",
      "64\n",
      "0.7788646117104713 0.9024154589371981 0.8210523128509521 0.834721565246582 0.8619416952133179\n",
      "0.6757059342638081 0.7939508506616257 0.7542498111724854 0.6579923033714294 0.804230809211731\n",
      "65\n",
      "0.8672750781222573 0.9583333333333334 0.8739511370658875 0.9313169121742249 0.8921328186988831\n",
      "0.8625228574042613 0.956821480406386 0.8535279035568237 0.8888949155807495 0.8672782778739929\n",
      "66\n",
      "0.6609710815583091 0.7830917874396135 0.8139547109603882 0.7962307929992676 0.8207849860191345\n",
      "0.6609710815583091 0.7830917874396135 0.786750853061676 0.7331022620201111 0.8069114685058594\n",
      "67\n",
      "0.7922055216299163 0.8958333333333334 0.7914612889289856 0.8129816651344299 0.7907578349113464\n",
      "0.7922055216299163 0.8958333333333334 0.7914612889289856 0.8129816651344299 0.7907578349113464\n",
      "68\n",
      "0.7592023424015524 0.8071988595866001 0.8024089932441711 0.7648915648460388 0.7875451445579529\n",
      "0.6920097332341509 0.7271285034373347 0.770368218421936 0.7648915648460388 0.7653208374977112\n",
      "69\n",
      "0.7227510748845007 0.8333333333333334 0.8338956236839294 0.8122463822364807 0.8395609855651855\n",
      "0.7227510748845007 0.8333333333333334 0.7725846171379089 0.7650167942047119 0.7537046670913696\n",
      "70\n",
      "0.666342926655142 0.7727272727272727 0.7674700617790222 0.7926945090293884 0.8290144205093384\n",
      "0.666342926655142 0.7727272727272727 0.7392789125442505 0.7926945090293884 0.7678431272506714\n",
      "71\n",
      "0.6587145824662664 0.7593582887700535 0.7929750084877014 0.7531002759933472 0.794536828994751\n",
      "0.6109050103943005 0.7045454545454546 0.734897255897522 0.6648815274238586 0.784756600856781\n",
      "72\n",
      "0.7774164580784383 0.8238095238095238 0.9203746914863586 0.8839393258094788 0.8203844428062439\n",
      "0.7774164580784383 0.8238095238095238 0.9203746914863586 0.8839393258094788 0.7985718250274658\n",
      "73\n",
      "0.8242707372153518 0.9087791495198903 0.8176884651184082 0.854961097240448 0.8140273690223694\n",
      "0.8242707372153518 0.9087791495198903 0.7901172637939453 0.852414608001709 0.8068932890892029\n",
      "74\n",
      "0.9746578663121909 0.9975961538461539 0.9619794487953186 0.9865511655807495 0.9770117402076721\n",
      "0.9746578663121909 0.9975961538461539 0.9619794487953186 0.9865511655807495 0.9571899771690369\n",
      "75\n",
      "0.6381478450023661 0.8 0.7995805740356445 0.7796930074691772 0.7941174507141113\n",
      "0.629264991208911 0.8 0.7995805740356445 0.6163621544837952 0.7423699498176575\n",
      "76\n",
      "0.7376501900713578 0.8712660028449503 0.8071216940879822 0.7367551326751709 0.7873138785362244\n",
      "0.7376501900713578 0.8712660028449503 0.7886135578155518 0.6733229756355286 0.7499431371688843\n",
      "77\n",
      "0.7101451603572487 0.8272058823529411 0.7725352644920349 0.7546663880348206 0.7630234360694885\n",
      "0.7101451603572487 0.8272058823529411 0.7725352644920349 0.721708357334137 0.7366938591003418\n",
      "78\n",
      "0.6195562283388104 0.7414965986394558 0.7155593633651733 0.7062324285507202 0.7470548748970032\n",
      "0.587531910073899 0.6428571428571429 0.6795554757118225 0.6373643279075623 0.7183129191398621\n",
      "79\n",
      "0.8455606055232273 0.9243918474687706 0.9153037667274475 0.9347782135009766 0.9048053622245789\n",
      "0.8455606055232273 0.9243918474687706 0.9153037667274475 0.9347782135009766 0.9048053622245789\n",
      "80\n",
      "0.7913361651328176 0.9010275824770146 0.8034770488739014 0.8036301136016846 0.8462596535682678\n",
      "0.7913361651328176 0.9010275824770146 0.7936217188835144 0.7999247908592224 0.8092743754386902\n",
      "81\n",
      "0.7402138903382278 0.8666666666666667 0.8137715458869934 0.8030961751937866 0.8177531361579895\n",
      "0.7402138903382278 0.8666666666666667 0.7799524068832397 0.8030961751937866 0.7647580504417419\n",
      "82\n",
      "0.7312664385557175 0.8704 0.8256995677947998 0.8149566650390625 0.8506971597671509\n",
      "0.7312664385557175 0.8704 0.7767295837402344 0.766903281211853 0.7962749004364014\n",
      "83\n",
      "0.784810497056072 0.9116666666666666 0.8444650173187256 0.8326159119606018 0.8751306533813477\n",
      "0.7222325450036584 0.850609756097561 0.8283165097236633 0.8018674254417419 0.8377019166946411\n",
      "84\n",
      "0.8001655996584696 0.9090909090909091 0.8408766388893127 0.7894310355186462 0.861243486404419\n",
      "0.8001655996584696 0.9090909090909091 0.8408766388893127 0.7894310355186462 0.8061452507972717\n",
      "85\n",
      "0.7315776480531151 0.8909090909090909 0.7702828049659729 0.8320146799087524 0.831809401512146\n",
      "0.7208897108832996 0.8690476190476191 0.7522135972976685 0.7707564234733582 0.8004633188247681\n",
      "86\n",
      "0.770867707499285 0.88125 0.862346887588501 0.8722757697105408 0.8522602915763855\n",
      "0.770867707499285 0.88125 0.8008577823638916 0.8331352472305298 0.8522602915763855\n",
      "87\n",
      "0.7815231388388586 0.8842975206611571 0.7828837037086487 0.7894929647445679 0.7784541845321655\n",
      "0.7815231388388586 0.8842975206611571 0.7792347073554993 0.7561671137809753 0.7784541845321655\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88\n",
      "0.8459415464041133 0.9125 0.8452849388122559 0.8847704529762268 0.8028877973556519\n",
      "0.8459415464041133 0.9125 0.7423678040504456 0.8459561467170715 0.7554154992103577\n",
      "89\n",
      "0.7992844010288475 0.9361344537815126 0.8292204737663269 0.8658027648925781 0.7807818055152893\n",
      "0.7992844010288475 0.9361344537815126 0.8292204737663269 0.8658027648925781 0.7807818055152893\n",
      "90\n",
      "0.6910521196939888 0.7869822485207101 0.8465554714202881 0.8984063863754272 0.7206754088401794\n",
      "0.6910521196939888 0.7869822485207101 0.8465554714202881 0.8132420778274536 0.7206754088401794\n",
      "91\n",
      "0.6911329564954034 0.8073089700996677 0.7928295731544495 0.7387921214103699 0.808596670627594\n",
      "0.5956097479092375 0.673469387755102 0.7521420121192932 0.7387921214103699 0.7695782780647278\n",
      "92\n",
      "0.8869328280532922 0.9594202898550724 0.8244200944900513 0.8290942907333374 0.8255105018615723\n",
      "0.8869328280532922 0.9594202898550724 0.8085594773292542 0.8247048854827881 0.7873662710189819\n",
      "93\n",
      "0.8723764137390219 0.9748148148148148 0.8753647208213806 0.8997042179107666 0.8876497745513916\n",
      "0.8723764137390219 0.9748148148148148 0.8625044822692871 0.8701609373092651 0.8647773265838623\n",
      "94\n",
      "0.7546443657957516 0.8918918918918919 0.8712494969367981 0.8919477462768555 0.889536440372467\n",
      "0.7546443657957516 0.8918918918918919 0.844570517539978 0.8778079152107239 0.8787306547164917\n",
      "95\n",
      "0.7533439977808334 0.8843537414965986 0.7705429792404175 0.7614632248878479 0.7822291254997253\n",
      "0.7533439977808334 0.8843537414965986 0.7705429792404175 0.7416409850120544 0.7414931654930115\n",
      "96\n",
      "0.8460634468224248 0.9396551724137931 0.8647364974021912 0.8402653932571411 0.8490718007087708\n",
      "0.8460634468224248 0.9396551724137931 0.8280539512634277 0.7764236330986023 0.8054677248001099\n",
      "97\n",
      "0.7133321984527776 0.8173076923076923 0.7610921263694763 0.7438121438026428 0.7901900410652161\n",
      "0.6909123284228068 0.7881944444444444 0.7610921263694763 0.7206977605819702 0.7901900410652161\n",
      "98\n",
      "0.6075827203138353 0.6703296703296703 0.7723627686500549 0.7256084084510803 0.7627928853034973\n",
      "0.6075827203138353 0.6703296703296703 0.7377275228500366 0.6723582148551941 0.7047027945518494\n",
      "99\n",
      "0.75566157801742 0.8801587301587301 0.8120359778404236 0.8398421406745911 0.8340983390808105\n",
      "0.75566157801742 0.8801587301587301 0.8120359778404236 0.8398421406745911 0.8340983390808105\n",
      "recall: 0.56\n",
      "precise 0.9491525423728814\n",
      "f1: 0.7044025157232704\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "ac=0\n",
    "# model=torch.load(\"model0.7304561298768334.pth\")\n",
    "# model=model.eval()\n",
    "ra=[]\n",
    "ra1=[]\n",
    "ratf=[]\n",
    "raidf=[]\n",
    "rarank=[]\n",
    "\n",
    "# for i in range(len(test1)):\n",
    "#     wordt_list1.append([])\n",
    "#     for j in range(len(test1[i])):\n",
    "#         x=test1[1][j].split(\" \")\n",
    "#         if maxl<len(x):\n",
    "#             maxl=len(x)\n",
    "#         x1= [word_to_idx[j1] for j1 in x]\n",
    "#         wordt_list1[-1].append()\n",
    "\n",
    "for i in range(len(test1)):\n",
    "    ra.append([])\n",
    "    ra1.append([])\n",
    "    ratf.append([])\n",
    "    raidf.append([])\n",
    "    rarank.append([])\n",
    "    for j in range(len(test2)):\n",
    "        test_1=[]\n",
    "        test_2=[]\n",
    "        for k in range(len(test1[i])):\n",
    "            for k1 in range(len(test2[j])):\n",
    "                test_1.append(test1[i][k])\n",
    "                test_2.append(test2[j][k1])\n",
    "        test_data=[]\n",
    "        maxl=0\n",
    "        for i1 in range(len(test_1)):\n",
    "            x=test_1[i1].split(\" \")\n",
    "            if maxl<len(x):\n",
    "                maxl=len(x)\n",
    "            x1= [word_to_idx[j1] for j1 in x]\n",
    "            x=test_2[i1].split(\" \")\n",
    "            if maxl<len(x):\n",
    "                maxl=len(x)\n",
    "            x2= [word_to_idx[j1] for j1 in x]\n",
    "            test_data.append(([x1,x2],1))\n",
    "\n",
    "        args['max_seq_len']=maxl\n",
    "        inputt1_len=[]\n",
    "        inputt2_len=[]\n",
    "        wordt_list1=[]\n",
    "        wordt_list2=[]\n",
    "\n",
    "        for i1 in range(len(test_data)):\n",
    "            word,tag= test_data[i1]\n",
    "            for j1 in range(len(word)):\n",
    "                if j1==0:\n",
    "                    inputt1_len.append(len(word[j1]))\n",
    "                else:\n",
    "                    inputt2_len.append(len(word[j1]))\n",
    "                while 1:\n",
    "                    if len(word[j1])<args['max_seq_len']:\n",
    "                        word[j1].append(0)\n",
    "                    else:\n",
    "                        break\n",
    "            wordt_list1.append(word[0])\n",
    "            wordt_list2.append(word[1])\n",
    "\n",
    "        inputts1=torch.LongTensor(wordt_list1)\n",
    "        inputts2=torch.LongTensor(wordt_list2)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            inputts1 = inputts1.cuda()\n",
    "            inputts2 = inputts2.cuda()\n",
    "\n",
    "        count1=0\n",
    "        count2=0\n",
    "        count3=0\n",
    "        count4=0\n",
    "        count5=0\n",
    "        for i1 in range(len(test_data)):\n",
    "            j1=len(inputts1[i1])-1\n",
    "            while 1:\n",
    "                if inputts1[i1,j1]==0:\n",
    "                    j1-=1\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "            j2=len(inputts2[i1])-1\n",
    "            while 1:\n",
    "                if inputts2[i1,j2]==0:\n",
    "                    j2-=1\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "            test_output = model(inputts1[i1,:j1+1].unsqueeze(0), inputts2[i1,:j2+1].unsqueeze(0))\n",
    "            \n",
    "            pred_y = torch.max(test_output.cpu(), 1)[1].data.numpy().squeeze()\n",
    "            if pred_y==1:\n",
    "                count2+=1\n",
    "            for i2 in range(len(test_output.cpu())):\n",
    "                count1+=F.softmax(test_output.cpu()[i2])[1].data.numpy().squeeze()\n",
    "            optimizer.zero_grad()\n",
    "        ra[-1].append(count1/len(test_data))\n",
    "        ra1[-1].append(count2/len(test_data))\n",
    "        \n",
    "        test_output = model(test1_att[i].unsqueeze(0),test2_att[j].unsqueeze(0))\n",
    "        for i2 in range(len(test_output.cpu())):\n",
    "            count3+=F.softmax(test_output.cpu()[i2])[1].data.numpy().squeeze()\n",
    "        \n",
    "        test_output = model(test3_att[i].unsqueeze(0),test4_att[j].unsqueeze(0))\n",
    "        for i2 in range(len(test_output.cpu())):\n",
    "            count4+=F.softmax(test_output.cpu()[i2])[1].data.numpy().squeeze()\n",
    "            \n",
    "        test_output = model(test5_att[i].unsqueeze(0),test6_att[j].unsqueeze(0))\n",
    "        for i2 in range(len(test_output.cpu())):\n",
    "            count5+=F.softmax(test_output.cpu()[i2])[1].data.numpy().squeeze()\n",
    "            \n",
    "        ratf[-1].append(count3)\n",
    "        raidf[-1].append(count4)\n",
    "        rarank[-1].append(count5)\n",
    "        \n",
    "    print(i)\n",
    "    print(max(ra[-1]),max(ra1[-1]),max(ratf[-1]),max(raidf[-1]),max(rarank[-1]))\n",
    "    print(ra[-1][i],ra1[-1][i],ratf[-1][i],raidf[-1][i],rarank[-1][i])\n",
    "\n",
    "ra=np.matrix(ra)\n",
    "rat=ra.T\n",
    "ac=0\n",
    "maac=0\n",
    "for i in range(len(ra)):\n",
    "    r1=np.argmax(ra[i])\n",
    "    r2=np.argmax(rat[i])\n",
    "    if r1==r2:\n",
    "        maac+=1\n",
    "    if r1==r2 and r1==i:\n",
    "        ac+=1\n",
    "recall=ac/len(ra)\n",
    "precise=ac/maac\n",
    "print(\"recall:\",recall)\n",
    "print(\"precise\",precise)\n",
    "print(\"f1:\",2*recall*precise/(recall+precise))\n",
    "# recall: 0.48936170212765956\n",
    "# precise 0.9583333333333334\n",
    "# f1: 0.6478873239436619"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sysu502/anaconda3/lib/python3.6/site-packages/torch/serialization.py:193: UserWarning: Couldn't retrieve source code for container of type DecAtt. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(model, 'LLG_DecAtt2.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open(\"/home/sysu502/Public/duxin/LLG1_rarank.txt\",\"w\",encoding=\"utf-8\")\n",
    "f.write(str(rarank))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "ra11=[]\n",
    "ra12=[]\n",
    "ra13=[]\n",
    "ra21=[]\n",
    "ra22=[]\n",
    "ra23=[]\n",
    "alpha=1\n",
    "for i in range(len(ra)):\n",
    "    ra11.append([])\n",
    "    ra12.append([])\n",
    "    ra13.append([])\n",
    "    ra21.append([])\n",
    "    ra22.append([])\n",
    "    ra23.append([])\n",
    "    for j in range(len(ra[i])):\n",
    "        ra11[-1].append(ra[i][j]*alpha+ratf[i][j]*(1-alpha))\n",
    "        ra12[-1].append(ra[i][j]*alpha+raidf[i][j]*(1-alpha))\n",
    "        ra13[-1].append(ra[i][j]*alpha+rarank[i][j]*(1-alpha))\n",
    "        ra21[-1].append(ra1[i][j]*alpha+ratf[i][j]*(1-alpha))\n",
    "        ra22[-1].append(ra1[i][j]*alpha+raidf[i][j]*(1-alpha))\n",
    "        ra23[-1].append(ra1[i][j]*alpha+rarank[i][j]*(1-alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.56 0.56 0.56 0.52 0.52 0.52\n",
      "0.9491525423728814 0.9491525423728814 0.9491525423728814 0.9285714285714286 0.9285714285714286 0.9285714285714286\n",
      "0.7044025157232704 0.7044025157232704 0.7044025157232704 0.6666666666666666 0.6666666666666666 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "count=100\n",
    "ra2=np.matrix(ra11[0:count][0:count])\n",
    "ra3=np.matrix(ra12[0:count][0:count])\n",
    "ra4=np.matrix(ra13[0:count][0:count])\n",
    "ra5=np.matrix(ra21[0:count][0:count])\n",
    "ra6=np.matrix(ra22[0:count][0:count])\n",
    "ra7=np.matrix(ra23[0:count][0:count])\n",
    "ac1=0\n",
    "maac1=0\n",
    "ac2=0\n",
    "maac2=0\n",
    "ac3=0\n",
    "maac3=0\n",
    "ac4=0\n",
    "maac4=0\n",
    "ac5=0\n",
    "maac5=0\n",
    "ac6=0\n",
    "maac6=0\n",
    "for i in range(len(ra2)):\n",
    "    r1=np.argmax(ra2[i])\n",
    "    r2=np.argmax(ra2[:,i])\n",
    "    r3=np.argmax(ra3[i])\n",
    "    r4=np.argmax(ra3[:,i])\n",
    "    r5=np.argmax(ra4[i])\n",
    "    r6=np.argmax(ra4[:,i])\n",
    "    r7=np.argmax(ra5[i])\n",
    "    r8=np.argmax(ra5[:,i])\n",
    "    r9=np.argmax(ra6[i])\n",
    "    r10=np.argmax(ra6[:,i])\n",
    "    r11=np.argmax(ra7[i])\n",
    "    r12=np.argmax(ra7[:,i])\n",
    "    if r1==r2:\n",
    "        maac1+=1\n",
    "    if r1==r2 and r1==i:\n",
    "        ac1+=1\n",
    "    if r3==r4:\n",
    "        maac2+=1\n",
    "    if r3==r4 and r3==i:\n",
    "        ac2+=1\n",
    "    if r5==r6:\n",
    "        maac3+=1\n",
    "    if r5==r6 and r5==i:\n",
    "        ac3+=1\n",
    "    if r7==r8:\n",
    "        maac4+=1\n",
    "    if r7==r8 and r7==i:\n",
    "        ac4+=1\n",
    "    if r9==r10:\n",
    "        maac5+=1\n",
    "    if r9==r10 and r9==i:\n",
    "        ac5+=1\n",
    "    if r11==r12:\n",
    "        maac6+=1\n",
    "    if r11==r12 and r11==i:\n",
    "        ac6+=1\n",
    "recall1=ac1/len(ra2)\n",
    "precise1=ac1/maac1\n",
    "recall2=ac2/len(ra2)\n",
    "precise2=ac2/maac2\n",
    "recall3=ac3/len(ra2)\n",
    "precise3=ac3/maac3\n",
    "recall4=ac4/len(ra2)\n",
    "precise4=ac4/maac4\n",
    "recall5=ac5/len(ra2)\n",
    "precise5=ac5/maac5\n",
    "recall6=ac6/len(ra2)\n",
    "precise6=ac6/maac6\n",
    "print(recall1,recall2,recall3,recall4,recall5,recall6)\n",
    "print(precise1,precise2,precise3,precise4,precise5,precise6)\n",
    "print(2*recall1*precise1/(recall1+precise1),2*recall2*precise2/(recall2+precise2),2*recall3*precise3/(recall3+precise3),2*recall4*precise4/(recall4+precise4),2*recall5*precise5/(recall5+precise5),2*recall6*precise6/(recall6+precise6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.LongTensor([[1,2,3],\n",
    "                 [4,5,6]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
